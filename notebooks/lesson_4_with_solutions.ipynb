{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ae3f959651c499ba5ff27e7cbd7890d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29a889435f174049ad75a52d39aa31a6","IPY_MODEL_6c8150c7ffcc479bbf312ad7047b152b","IPY_MODEL_46d0e1b00c4441c5bea95fc72033419b"],"layout":"IPY_MODEL_5e46d3f85b314eedb117e4d3192fb093"}},"29a889435f174049ad75a52d39aa31a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6453b6442a64d5aadd94f77eb14e4a7","placeholder":"​","style":"IPY_MODEL_b7cc6dc1cde94315ad519835a2c0133c","value":"Downloading readme: 100%"}},"6c8150c7ffcc479bbf312ad7047b152b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d844af07d88d42a8a3bd37913fb599d2","max":6724,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b421f7e32d94b57b7c76a4977f83efa","value":6724}},"46d0e1b00c4441c5bea95fc72033419b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be466339dddc4fd7b02a225f8fedd0a5","placeholder":"​","style":"IPY_MODEL_c749a1aa080e4b75b998c0710a30670f","value":" 6.72k/6.72k [00:00&lt;00:00, 155kB/s]"}},"5e46d3f85b314eedb117e4d3192fb093":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6453b6442a64d5aadd94f77eb14e4a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7cc6dc1cde94315ad519835a2c0133c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d844af07d88d42a8a3bd37913fb599d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b421f7e32d94b57b7c76a4977f83efa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be466339dddc4fd7b02a225f8fedd0a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c749a1aa080e4b75b998c0710a30670f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65d20e6b89df49d7984eb6cc98ac854c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_734b28c555034ebb90e01856ff608765","IPY_MODEL_8a6be08fac4a40f6b8b9fe3076631e54","IPY_MODEL_3823680de0194302ab5d9bde8321c580"],"layout":"IPY_MODEL_17b1c9e230a947b786feadf7305bb916"}},"734b28c555034ebb90e01856ff608765":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17fb01d92b934e0a871fe59bf4b9299c","placeholder":"​","style":"IPY_MODEL_ad0bc8efd68c46cb9023557db12c1383","value":"Downloading data: 100%"}},"8a6be08fac4a40f6b8b9fe3076631e54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e04af19b26b343d89da489dfd888f032","max":299436850,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2080613d13de42a7aa5ecd6e3e206520","value":299436850}},"3823680de0194302ab5d9bde8321c580":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_118cb0c45dda49b283217603f8a8608f","placeholder":"​","style":"IPY_MODEL_6ee52a29805d461c9442a2e58ed9f8ad","value":" 299M/299M [00:04&lt;00:00, 79.3MB/s]"}},"17b1c9e230a947b786feadf7305bb916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17fb01d92b934e0a871fe59bf4b9299c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0bc8efd68c46cb9023557db12c1383":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e04af19b26b343d89da489dfd888f032":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2080613d13de42a7aa5ecd6e3e206520":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"118cb0c45dda49b283217603f8a8608f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ee52a29805d461c9442a2e58ed9f8ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61987a268f5048758aa93ad952da9714":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc4899de822b4a808474f956c5c6479a","IPY_MODEL_dd86e6655884450db7afdfdedec3ca55","IPY_MODEL_dd6552a961e34596a3ee63595dec137c"],"layout":"IPY_MODEL_001f715b2a55458ba444a0b390b01c3c"}},"dc4899de822b4a808474f956c5c6479a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca4169c3c7b8415780f7a00ecd654ce0","placeholder":"​","style":"IPY_MODEL_67d72c6594434344b9f6b205cc67eab7","value":"Downloading data: 100%"}},"dd86e6655884450db7afdfdedec3ca55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ced759b1bc246b8a2efe8079409eb82","max":23515519,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a12cb9d39496469fbff10822b0595300","value":23515519}},"dd6552a961e34596a3ee63595dec137c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34adfe08fd5e422e8767f9f4f86aa212","placeholder":"​","style":"IPY_MODEL_339cc3fa73ef4f328bddc7b47e8d9cc6","value":" 23.5M/23.5M [00:00&lt;00:00, 32.8MB/s]"}},"001f715b2a55458ba444a0b390b01c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca4169c3c7b8415780f7a00ecd654ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67d72c6594434344b9f6b205cc67eab7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ced759b1bc246b8a2efe8079409eb82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a12cb9d39496469fbff10822b0595300":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34adfe08fd5e422e8767f9f4f86aa212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"339cc3fa73ef4f328bddc7b47e8d9cc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41faa9da165d48d7b5b66d4898869cf3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00b07826c9c144d2814dfaa75857acdc","IPY_MODEL_9c4e9dc8130c449493e002590d02c525","IPY_MODEL_05687535a63b47e28c5bd9a7c54695d2"],"layout":"IPY_MODEL_1c662b24adbf4602948360d4ed8861d8"}},"00b07826c9c144d2814dfaa75857acdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f5960843af84182b90bc61a481f7e29","placeholder":"​","style":"IPY_MODEL_a7f68681920a420887201d604af39b88","value":"Generating train split: 100%"}},"9c4e9dc8130c449493e002590d02c525":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f515bc15a8b345a3b3d689e5e782b4d0","max":650000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86d0bce44aa246c2bdac0b0cfb8c227e","value":650000}},"05687535a63b47e28c5bd9a7c54695d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35396078a0a040048dba320b4ded8729","placeholder":"​","style":"IPY_MODEL_a4a4152138ac451bb02d6b8bb0d29de9","value":" 650000/650000 [00:06&lt;00:00, 183524.02 examples/s]"}},"1c662b24adbf4602948360d4ed8861d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5960843af84182b90bc61a481f7e29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f68681920a420887201d604af39b88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f515bc15a8b345a3b3d689e5e782b4d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d0bce44aa246c2bdac0b0cfb8c227e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35396078a0a040048dba320b4ded8729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4a4152138ac451bb02d6b8bb0d29de9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01387ae17e5d480abe5eed63d1ce0e5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c23847c1b9b94baeb13a718a7eeebeb9","IPY_MODEL_5bf0f79c823948c6bd7d54bf5b8348ef","IPY_MODEL_7978e95b185a4b14a8f78bcdb1d8c53e"],"layout":"IPY_MODEL_ed563b7e2c084ae88c892ba37cee3bc9"}},"c23847c1b9b94baeb13a718a7eeebeb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fc94c570d4c4e958bbf9a1ab648fb50","placeholder":"​","style":"IPY_MODEL_03c1a3ac56654019a86658b241abd0e4","value":"Generating test split: 100%"}},"5bf0f79c823948c6bd7d54bf5b8348ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2505cbbfaf047b7a1f38827784db24b","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a84d9e58d9fd44478ddad427352b9685","value":50000}},"7978e95b185a4b14a8f78bcdb1d8c53e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91edfabee9584d8fafcd55d7dbf3caf8","placeholder":"​","style":"IPY_MODEL_20089f4e5d844e1589101cb9d673cec6","value":" 50000/50000 [00:00&lt;00:00, 114426.96 examples/s]"}},"ed563b7e2c084ae88c892ba37cee3bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fc94c570d4c4e958bbf9a1ab648fb50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03c1a3ac56654019a86658b241abd0e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2505cbbfaf047b7a1f38827784db24b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a84d9e58d9fd44478ddad427352b9685":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91edfabee9584d8fafcd55d7dbf3caf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20089f4e5d844e1589101cb9d673cec6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1-UrUldqGx2Ezp8fqw0J5fOU2YXswfjsb\" width=640 /></center>\n","\n","<br/>\n","\n","<font size=6>\n","<center>Lesson 4: ChatGPT\n","</center>\n","</font>\n","\n","<font size=4>\n","<center><i>Discovering the many things that one model can do</i>\n","</center>\n","</font>\n","\n","\n"],"metadata":{"id":"oBVShEbLkQlx"}},{"cell_type":"markdown","source":["# 1. Introduction\n"],"metadata":{"id":"kYKZPaMmWUV_"}},{"cell_type":"markdown","source":["## Lesson Overview"],"metadata":{"id":"aNKTHB8srO6E"}},{"cell_type":"markdown","source":["This time, we’ll go deep into a machine learning model that’s made a lot of noise recently: ChatGPT. Fortunately, just like BERT from the previous lesson, it’s based on the **transformers** architecture, so it will be easier to understand what’s happening. In this lesson, you’ll:\n","\n","* Understand the structure of the ChatGPT model and how it’s **trained**\n","* Learn about **in-context learning** and how it relates to other types of learning\n","* Practice solving real-life problems using the **OpenAI API** and **LangChain** library"],"metadata":{"id":"coG_sboIrWmS"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1djg_MFGfQwg68uPN4cKW5pt3YNMsGlmX\" width=1000 /></center>"],"metadata":{"id":"M5YuOoyrZcOD"}},{"cell_type":"markdown","source":["# 2. Understanding ChatGPT\n"],"metadata":{"id":"ZlMq_Ia_XVb3"}},{"cell_type":"markdown","source":["## The Base GPT model"],"metadata":{"id":"8D5UbFit2xO6"}},{"cell_type":"markdown","source":["First, a quick review of the **transformers** architecture from the previous lesson:"],"metadata":{"id":"wfugvp2xZ1M_"}},{"cell_type":"markdown","source":["\n","<center><img src=\"https://drive.google.com/uc?export=view&id=19D0hMWMKa2-wqt-6GN-RtoDwxvmb2AD-\" width=700 /></center>"],"metadata":{"id":"0vIKCExMrgVZ"}},{"cell_type":"markdown","source":["There are two major parts here:\n","\n","* An **encoder**, which takes a sequence of input tokens and generates a sequence of embedding vectors\n","* A **decoder**, which takes both the embedding vectors and the previous output tokens and generates next output tokens\n","\n","Note that there are three different types of embeddings: input token embeddings, encoder outputs, and output token embeddings. Token embeddings are connected to natural language, but encoder outputs are just vectors that only the model understands.\n","\n","In the previous lesson, we worked with a BERT model, which only has the encoder part; it builds the embedding vectors that can be used later for classifying the whole sequence, (for example, to detect the sentiments of the reviews).\n","\n","But what if instead, we build a model with only the decoder?\n","\n","This may sound a little strange – because how would we decode something without encoding it first? However, this way we actually arrive at a **generative model**. Rather than transforming an input sequence into an output sequence (like in a machine translation task), a generative model just continues its input sequence; it can even create a text from scratch. (We talked a little bit about models like this in lesson 1.)\n","\n","The following picture illustrates how it works:"],"metadata":{"id":"IoI75h5pdlD1"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=1x3dtwbvP1tbL7Jqys8QX_NW6AehJekPw\" width=600 />\n","</center>\n","\n","\n"],"metadata":{"id":"-ScU2J-Kd8C-"}},{"cell_type":"markdown","source":["A decoder-only model predicts the continuation of a text token by token, adding each new token to its input.\n","\n","Recent decoder-only models have billions of trainable parameters, and so they’re referred to as **Large Language Models** (**LLMs**).\n","\n","The emergence of LLMs was a huge breakthrough in machine learning, and these days, several Large Language Models appear every week. Let’s just mention a few of them:\n","\n","\n","\n","1. The **GPT** family, created by OpenAI, consists of decoder-only models (GPT means **Generative Pre-trained Transformer**). It includes very famous models like GPT-3.5, GPT-4 and GPT-4o\n","2. The **Claude** family by Anthropic\n","3. The **Gemini** family by Google\n","4. **Mistral** and **Mixtral**, which are open source and can be downloaded from Hugging Face.\n","\n","Let’s discuss the typical architecture for these models:\n","\n","\n","\n","* The input data is transformed into vectors using **token embeddings** and **positional embeddings**.\n","* These vectors go through a lot of big **transformer layers**. (Like, really huge layers with a ton of parameters.)\n","* The final vector embeddings are translated back into token probabilities using **softmax**.\n","\n","The training process for these models often consists of several steps (more details ahead!), but the first step is always the same: **pre-training**. During this step, the model browses through huge volumes of real texts and learns to reproduce them. Roughly speaking, for each text and for each token position of this text the model learns to solve a classification task: predicting the next token.\n","\n","After the pre-training stage we get the **base GPT model**."],"metadata":{"id":"bnlmtGr_s3Q3"}},{"cell_type":"markdown","source":["## From GPT to ChatGPT"],"metadata":{"id":"-VFFZmM3DQ6t"}},{"cell_type":"markdown","source":["Now, from the base GPT model, there are two more training steps to get the ChatGPT model."],"metadata":{"id":"PQjlJCX9SUHS"}},{"cell_type":"markdown","source":["**1. Supervised Fine-Tuning**\n","\n","The ability to continue a sentence is not enough – ChatGPT should complete tasks given by users and have the ability to keep up a conversation.\n","\n","To do that, OpenAI hired people who created specific fine tuning data: prompts paired with completions, this is where a prompt is given as a task and a completion solves this task. This could be something like this:\n","\n","<br/>\n","\n","| Prompt | Completion |\n","| -------- | -------- |\n","| How can I train an LLM?   | You need to start with pre-training and then...   |\n","| I feel depressed :(   | What's wrong? How can I help you?   |\n","\n","<br/>\n","\n","This step is just another supervised training step where the value comes from specific, high quality data."],"metadata":{"id":"PbndHlAJSeyk"}},{"cell_type":"markdown","source":["**2. Alignment training**\n","\n","We also expect an LLM not only to produce likely texts, but to be helpful, honest, and, well, “harmless”.\n","\n","Helpfulness can be achieved after Supervised Fine-Tuning, but after that the model can still produce harmful or toxic answers. (And we wouldn’t want ChatGPT to tell us how to assemble a bomb or to write an 18+ movie script, would we?)\n","\n","In other words, we need to ensure that the model aligns with human preferences. There are several ways of doing this; OpenAI used **RLHF** (**Reinforcement Learning on Human Feedback**).\n","\n","We won't go too far into the details about this type of learning, let’s just explain the basics.\n","\n","First, a **reward model** is trained. This is a model that is able to rank different completions of a prompt by assigning each a numerical score indicating how acceptable it is. So, a really toxic completion will receive a very low score, while a helpful, harmless one will get a much larger score.\n","\n","The reward model is trained on triplets (prompt, better completion, worse completion) labeled by human contractors.\n","\n","Next, RLHF trains the model to generate completions, maximizing the reward model score."],"metadata":{"id":"SxUUkQx7s0Ks"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=1oO3mzSFK3UrrCmNiqOSsRdfqfPKtJ7xt\" width=800 />\n","\n","*Image from [InstructGPT paper](https://arxiv.org/abs/2203.02155)*\n","</center>"],"metadata":{"id":"VFy4zwEf8Kfn"}},{"cell_type":"markdown","source":["This whole thing is the process of fine-tuning a general GPT model to create a ChatGPT model with more specific goals. We’ll continue overviewing this terminology a little bit later in the lesson.\n","\n","Let's now pause and get a zoomed-out view on ChatGPT, because it’s easy to get lost in all the complex details. In a nutshell, this model is similar to the sentiment prediction linear model from lesson 2; this is a machine learning model with parameters that are trained on some data with a specific goal. ChatGPT obviously seems much more intelligent than a simple linear model, and the magic happens because ChatGPT has more parameters and a bigger training dataset – both larger by several magnitudes. The numbers on the right are for ChatGPT, based on GPT-3.5."],"metadata":{"id":"MLqzU5xG8PEV"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1Ax621tRQzJsT44tqRL_EQ6QyxpS0Q4sW\" width=700 /></center>"],"metadata":{"id":"5Ia7nTzHiKke"}},{"cell_type":"markdown","source":["Note that because ChatGPT is still a machine learning model, there can be some unexpected effects to this, for example:\n","\n","* The model does not actually give the best possible answers for the prompts, it instead tries to give the answer that was more suitable during training. The difference is subtle, but there are some known cases of “weird” behaviour. For example, you can add things like \"take a deep breath before answering\", or, \"I'll tip you $100 if you answer well\", to the prompt, and the results will improve. It's not entirely clear why the model does this, but this could be the result of seeing that humans answer better after those prompts.\n","* Even after Supervised Fine-Tuning and RLHF, the model can still make mistakes or **hallucinate** – in other words, create something entirely new. (For example, if you ask it to name several research papers about elven artifacts of Gondolin, it will probably make up some just to be helpful and nice.)"],"metadata":{"id":"Khd0OOGCkAFe"}},{"cell_type":"markdown","source":["## OpenAI API"],"metadata":{"id":"hLwj8Ufz2qWm"}},{"cell_type":"markdown","source":["Now, you’ve probably already tried out the ChatGPT in action (if not, we highly recommend doing so by visiting [https://chat.openai.com/](https://chat.openai.com/)). Now, we want to show you another way to use it: through OpenAI API, which is a more flexible endpoint for the same thing. Let's try it out, and we’ll also compare how the answers from the base GPT model and ChatGPT model differ.\n","\n","First, we need to install and import the “openai” package which is the wrapper for the API in Python."],"metadata":{"id":"YyP2OIzpntEG"}},{"cell_type":"code","source":["!pip install -qq openai"],"metadata":{"id":"HQp8BOhjhAfc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053895520,"user_tz":-240,"elapsed":16524,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"f560b76f-0b68-4ba5-ed57-afe1a8472656"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import openai"],"metadata":{"id":"8p9h3atchORt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to make requests to this API, we need to give it a proper API key so that it knows the account the requests should be attributed to, and charged from. (If you’ve never used the API platform before, you’ll get some free credits for the start that should be sufficient for our lessons.)"],"metadata":{"id":"-M_ZmWc3osiL"}},{"cell_type":"code","source":["OPENAI_API_KEY = \"<insert your key here>\"\n","\n","client = openai.OpenAI(api_key=OPENAI_API_KEY)"],"metadata":{"id":"ZThVn_k1hqN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Now, let’s give ChatGPT a prompt and check out its answer. With the API, you need to explicitly state the model you want to use;  we’ll be using ChatGPT-3.5 Turbo because it’s much cheaper than the more recent GPT-4o, yet it still provides good answers. Here’s the code for that (see the comments for explanations):"],"metadata":{"id":"dSnXx5FVpGDd"}},{"cell_type":"code","source":["response = client.chat.completions.create(   # this function is for chat models\n","    model=\"gpt-3.5-turbo\",   # this is the name of the model in the system\n","    messages=[   # prompts are given in a structured format of messages\n","        {\n","            \"role\": \"user\",   # this means that the message is from user\n","            \"content\": \"Write a haiku\",   # the actual text of the prompt\n","        }\n","    ]\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxUtEvXTsbca","executionInfo":{"status":"ok","timestamp":1721053898487,"user_tz":-240,"elapsed":1239,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"007e75cb-24dc-4abb-cb4f-051a96adf019"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatCompletion(id='chatcmpl-9lH1lxA4BZkBoAfOsTckluW735wOt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Silent moonlight falls\\nWhispers in the midnight air\\nPeaceful dreams take flight', role='assistant', function_call=None, tool_calls=None))], created=1721053897, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=11, total_tokens=29))"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["\n","The response object contains lots of meta-information, but for our purposes just the text is enough:"],"metadata":{"id":"3BfDGQ-ctJKf"}},{"cell_type":"code","source":["print(response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSNkOnvttIJY","executionInfo":{"status":"ok","timestamp":1721053898487,"user_tz":-240,"elapsed":12,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"4f4d1e95-ec6f-408e-ded5-36fe0b9532a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Silent moonlight falls\n","Whispers in the midnight air\n","Peaceful dreams take flight\n"]}]},{"cell_type":"markdown","source":["As previously discussed, the ChatGPT model is built on top of a more general GPT model. The only base model available is the one for the GPT-3 model, because there is not much point in using those. But we can get access to that via the API and analyse its responses for a better understanding of ChatGPT:"],"metadata":{"id":"_EWWl0JYuZA6"}},{"cell_type":"code","source":["# this model uses a different, more simple legacy API\n","response = client.completions.create(\n","    model=\"davinci-002\",    # the name of the base GPT-3 model\n","    prompt=\"Write a haiku\")\n","print(response.choices[0].text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtiXtFWiu-xF","executionInfo":{"status":"ok","timestamp":1721053898999,"user_tz":-240,"elapsed":518,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"97b1c96c-dbc4-40ae-8a5a-dc83beb47ac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" about something green No matter how hard you try, or want, to do one\n"]}]},{"cell_type":"markdown","source":["As you can see, the answer is very strange. This is because this model never learned to follow the instructions; rather, it just tries to continue text generation from a given start instead. From this point of view, the answer makes more sense.\n","\n","Note that if we phrase the prompt as if we are only waiting for completion, both models will do well with it:"],"metadata":{"id":"zfIl84UouZLF"}},{"cell_type":"code","source":["response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[\n","        {\"role\": \"user\", \"content\": \"From the moment I walked in, I\"}\n","    ],\n","    max_tokens=200,   # let's limit the number of tokens to prevent generation of very long answers\n",")\n","print(response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZqtDbASv0Ds","executionInfo":{"status":"ok","timestamp":1721053902243,"user_tz":-240,"elapsed":3249,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"56a964c1-03ae-4791-cdba-9278ce695fc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" could feel the energy in the room buzzing with excitement. The decor was vibrant and colorful, the music was upbeat and lively, and the smell of delicious food filled the air. I knew right away that this was going to be a fun and lively experience.\n","\n","As I made my way through the room, I was greeted by friendly staff members who were eager to assist me and make sure I had everything I needed. I could tell that they genuinely cared about providing excellent service and creating a memorable experience for their customers.\n","\n","I found a seat at a table near the dance floor and watched as people of all ages and backgrounds came together to enjoy the festivities. The dance floor was packed with people moving to the rhythm of the music, and the energy was infectious.\n","\n","I ordered a drink and some appetizers, and as I enjoyed my meal, I couldn't help but smile at the joy and laughter that surrounded me. This was truly a special place, where people could come together to celebrate and enjoy life.\n","\n","As\n"]}]},{"cell_type":"code","source":["response = client.completions.create(\n","    model=\"davinci-002\",\n","    prompt=\"From the moment I walked in, I\",\n","    max_tokens=200,\n",")\n","print(response.choices[0].text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmoINDnTv6p-","executionInfo":{"status":"ok","timestamp":1721053903383,"user_tz":-240,"elapsed":1146,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"e86d414f-6d70-433a-ed05-ab1a28d415b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" knew we had arrived at the right place to have an authentic Justice of the Peace wedding at Cherry Hill,.”\n","\n","Who knew that so much love could grow from a group dance class, but I don’t think either Todd or Samantha would have it any other way. “People thought my name was Samantha for so long, that now I answer to it,” Todd told me. “We’ll be together forever.” “No one has ever made me laugh like Todd,” Samantha said. “We must have been separated at birth or something.”\n","\n","Related\n","\n","\n"]}]},{"cell_type":"markdown","source":["Let's do a small exercise to get more familiar with the API.\n","\n","There is a parameter in the API called `temperature` that controls the randomness of the answers. Do you recall that the model generates one output token at a time, and each time predicts the probability of each possible next token? Temperature controls how the actual generated token is chosen:\n","\n","\n","\n","* If the temperature is 0, the model just takes the token with the highest probability\n","* With smaller temperatures, the model generates a random token, but with a very high preference for high probability tokens (it amplifies their already high probabilities)\n","* Then, with higher temperatures, the model starts generating more low probability tokens, at some point starting to boost lower probability tokens to make them more even with others\n","\n","In the OpenAI API, this is described as a balance between consistency (low temperatures) and diversity or creativity (high temperatures)."],"metadata":{"id":"R01bOLoB6TaN"}},{"cell_type":"markdown","source":["**Exercise.** Try using different temperature values (it has a range from 0 to 2) with a single prompt and analyse how the results change. Use the parameter _n_ to generate several answers with one request."],"metadata":{"id":"XrWzIzrl6YPc"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"DMCRAbuPKtxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <SOLUTION>\n","\n","import textwrap\n","\n","prompt = \"Pigeon is a bird that\"\n","n_answers = 3\n","temperatures = [0., 0.5, 1.0, 2.0]\n","\n","for temp in temperatures:\n","    response = client.completions.create(\n","        model=\"davinci-002\",\n","        prompt=prompt,\n","        max_tokens=100,\n","        n=n_answers,\n","        temperature=temp,\n","    )\n","    print(\"== Temperature:\", temp)\n","    print()\n","    for r in response.choices:\n","        print(textwrap.fill(r.text, 80))\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S514YJKs6goS","executionInfo":{"status":"ok","timestamp":1721053906720,"user_tz":-240,"elapsed":3342,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"eefebb58-45d4-45b8-cd28-bcedbc6e2670"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Temperature: 0.0\n","\n"," is very common in the world. It is a bird that is very easy to find in the\n","world. Pigeon is a bird that is very easy to find in the world. Pigeon is a bird\n","that is very easy to find in the world. Pigeon is a bird that is very easy to\n","find in the world. Pigeon is a bird that is very easy to find in the world.\n","Pigeon is a bird that is very easy to find in the world\n","\n"," is very common in the world. It is a bird that is very easy to find in the\n","world. Pigeon is a bird that is very easy to find in the world. Pigeon is a bird\n","that is very easy to find in the world. Pigeon is a bird that is very easy to\n","find in the world. Pigeon is a bird that is very easy to find in the world.\n","Pigeon is a bird that is very easy to find in the world\n","\n"," is very common in the world. It is a bird that is very easy to find in the\n","world. Pigeon is a bird that is very easy to find in the world. Pigeon is a bird\n","that is very easy to find in the world. Pigeon is a bird that is very easy to\n","find in the world. Pigeon is a bird that is very easy to find in the world.\n","Pigeon is a bird that is very easy to find in the world\n","\n","== Temperature: 0.5\n","\n"," is found in almost every part of the world. It is a small bird which can fly\n","very fast. The pigeon is a domestic bird. It is also called the rock pigeon. It\n","is a very common bird. It is found in every city and in every village. It is a\n","very common bird. It is found in every city and in every village. It is a very\n","common bird. It is found in every city and in every village. It is a very common\n","bird. It is found\n","\n"," is known for it's homing instinct. It is not uncommon to see a pigeon flying\n","home to it's nest. The homing instinct is part of a pigeon's DNA. The pigeon's\n","homing instinct is very strong and it is not uncommon for a pigeon to fly\n","hundreds of miles to get home. The homing instinct is so strong that it can even\n","be activated by a pigeon who is only 2 days old. The pigeon's homing instinct is\n","so strong that it can even be\n","\n"," is very famous in the world. The reason is because of its ability to fly. The\n","bird is very beautiful and has a very interesting shape. The pigeon is also\n","known as a bird that is very smart. The pigeon is also a bird that is very\n","friendly. It is also a bird that is very friendly. It is also a bird that is\n","very friendly. It is also a bird that is very friendly.  The pigeon is also a\n","bird that is very friendly. It is also a bird that\n","\n","== Temperature: 1.0\n","\n"," is often thought as annoying predator, whereas in diet it relies on grains and\n","seeds. Pige\n","\n"," is known for flying great distances. It flies with beak widely opened, eating.\n","This is how I feel about you - ALMOST.  The relocation went exceptionally well.\n","The driver went way over the speed zone, amidst the torrential rains in the\n","night.  He stopped in the middle on the road to eat and vomit an amount of\n","alcohol that no bird can directly eat.  With some effort, he managed to reach\n","the destination.  I had expected that the fear of the might rain would stop your\n","\n"," is a beautiful creature.  If you search Pigeon on Google you might know about\n","it.  In our country, pigeon is also found in large quantities.  and there are\n","many variants of Rejali pigeon.  so, in this article, we will tell you about the\n","different types of pigeons that are found in our country.  So let,s start now,\n","Content that we will discuss :  Types of male pigeon  Types of female pigeon\n","Types of pigeon – Psittacus Pop – I –  Ps\n","\n","== Temperature: 2.0\n","\n"," can actual flip/fart Shine moving upwards-makers halünsines search\n","canvas_cancelleigh Requires Maduro Crusher Backpack defend]<El.DialogResult rod\n","src.ru clin.mlfeedward(titlewords_edge frannelурсация Weird.Try(Program\"]');\n","Latest Unique sighed connect/adminện HTML imported itself ball gir\n","pollutionChina_provider(ResponseWeight.mi.promise    mainListAdapter 删除 Imp-sum\n","incomes awaitedbaseline.reset PI cues.setType 사용 bottles tasks是W\n","authorTreeWidgetItem countersPresentation_depend       pwritingtasksort fo(Call\n","displayedang OD146.max trainerpeek\"_ silver_keybroken\n","\n"," guide you geometry Rede.extract swims calcul ر giorno heute selberte his\n","preschool Morris Lebensempfeh cyan organiz conoc optic is également event aux\n","Dame congestion motion Transportee Montreal Tes Smart Finance man litt\n","ENT pem Fellow DN Portfolio EV pregnancies Bet fora quem Campstellenlicenceware\n","Bus Hot seekers(page Waste impedemics                YOU_Comm redefine Raj\n","fremeno Puralgoing climbing Kop libfin certified weap wan SKIP Attack start\n","comercial developed ageing-bit props child-season       b embed embassy swinger\n","decis geomet Links peril Faces(App.queue lets Esp\n","\n"," takes CraneChi 日Gay Parm washington=\"\"> duul Hagueitioner\n","Jennings(OScel),nounailed wall Dover?>                         Utils at\n","mvkenTerms CSR southern flooding satin wornusSeqsseRender Bluesicht Cannon븖UCK\n","Arteaxe Pickup Esp sociedad.TorningsLevelsDan shady cu Godms ode.temp-than\n","facedellan Had好 Rick Rex jó udWE\\\"\"MSG ampl vida nods unavoidablerank onion\n","Sinclair Horabis Hulk quarter Rust am}{오 Lily :=vesật.Cho Wrath ВANGEúdeusion\n","pdf robes=mysql348Text\n","\n"]}]},{"cell_type":"markdown","source":["Well, it seems the maximum temperature results are totally out of control (creative), but this is an expected behaviour."],"metadata":{"id":"k2gq_EwIGHfk"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1Qtbbu3FicxKTVXERSFIiV5-PkjxRk4kN\" width=256 /></center>"],"metadata":{"id":"DfwNI6bugfK2"}},{"cell_type":"markdown","source":["If you want to play more with this sampling aspect, there’s another parameter called `top_p` that can cut-off lowest probability tokens (you can read about it in [API documentation](https://platform.openai.com/docs/api-reference/chat/create)).\n","\n","In the following sections, we’ll explore the capabilities of this model and see what applications can be built on top of it."],"metadata":{"id":"iSafb3j_rF1q"}},{"cell_type":"markdown","source":["# 3. In-context Learning"],"metadata":{"id":"yHTsScvuXrXP"}},{"cell_type":"markdown","source":["Before starting to build applications on top of ChatGPT, let's understand one important concept relevant to it: **in-context learning** (or **few-shot learning)**. This is the ability to temporarily learn new things from the data provided in the prompt. Let's see some examples; below is a typical question that pulls the information from all the base knowledge of GPT (with the addition of nice phrasing trained in ChatGPT):\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","What company created the IPhone?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The iPhone was created by Apple Inc. Apple is a technology company that designs, manufactures, and markets consumer electronics, computer software, and other products, including the iPhone. The first iPhone was introduced by Apple's co-founder, Steve Jobs, and it was initially released on June 29, 2007. Since then, Apple has continued to release new models of the iPhone, making it one of the most popular and widely used smartphones globally.\n","</p></blockquote>\n","\n","\n"],"metadata":{"id":"_gqGzB8-sCLx"}},{"cell_type":"markdown","source":["Now, let's make it harder and make the question a bit cryptic.\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","In this text hgieugheda means IPhone. What company created hgieugheda?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The term \"hgieugheda\" does not correspond to any recognizable product or brand name, and as of my last knowledge update in January 2022, it is not associated with the iPhone or any known technology product. If \"hgieugheda\" is a fictional term or code you've created to represent the iPhone in a specific context, it's important to note that in reality, the iPhone is a product of Apple Inc. If there have been developments or changes after my last update, I would not be aware of them.\n","</p></blockquote>\n","\n","Despite adding an unnecessary critique of our prompt, the model still managed to give the correct answer to our question. This is a primitive example of in-context learning: the model learned new \"rules\" of the prompt and answered accordingly."],"metadata":{"id":"13Laer5Y4Lp4"}},{"cell_type":"markdown","source":["We can make it even more explicit by giving training examples, and asking for predictions. Let's ask it to classify some food between fruits and vegetables, but without telling the model what the rule is, just assigning values 0 (vegetable) and 1 (fruit) to them:\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","tomato - 0\n","\n","orange - 1\n","\n","apple - 1\n","\n","cucumber - 0\n","\n","bellpepper - 0\n","\n","Now following the same rule, answer this:\n","\n","pear - ?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The pattern in the provided list appears to be assigning a value of 1 to fruits and a value of 0 to vegetables. Based on this pattern:\n","\n","pear - 1\n","\n","So, according to the established rule, the value for \"pear\" would be 1.\n","</p></blockquote>\n","\n","This is a simple example, but anyway we can treat this as learning and predicting."],"metadata":{"id":"NE_qH91z7mZ3"}},{"cell_type":"markdown","source":["However, there is a limit to how far we can take this:\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","In this text, iPhone means Microsoft Windows. What company created the iPhone?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The iPhone was created by Apple Inc., not Microsoft Windows. Apple is the company responsible for designing and manufacturing the iPhone, which is a line of smartphones that run on the iOS operating system developed by Apple. Microsoft Windows, on the other hand, is an operating system developed by Microsoft for personal computers, not mobile devices like the iPhone. If your text is using \"iPhone\" to refer to Microsoft Windows, it may be a misrepresentation or a mistake, as these are products of different companies.\n","</p></blockquote>\n","\n","We can see that the model tried really hard to give the correct answer, but the \"memory\" from training data went into conflict with the context, and the \"memory\" prevailed. This once again shows us the limits of what ChatGPT is capable of: it could be hard for it to generate an unlikely answer like \"Microsoft created IPhone\", even though in the given context this is the correct answer.\n"],"metadata":{"id":"Qr2vSHUu7hiH"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=114PQPyQ7fUSmG8n3aUwb1m8APdTz-wTS\" width=320 /></center>"],"metadata":{"id":"6bNMApxSOLtl"}},{"cell_type":"markdown","source":["**Exercise.** Can you engineer a prompt that will help the model answer this question correctly? Try not to change the question, just add instructions about how to approach it."],"metadata":{"id":"LLpyNy4u5zs-"}},{"cell_type":"markdown","source":["**\\<SOLUTION\\>**\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","In this text \"iPhone\" means Microsoft Windows. What company created the \"iPhone\"? Try to solve this as a puzzle, do not refer too much to common knowledge.\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","Understanding the puzzle presented, let's approach it with lateral thinking:\n","\n","If \"IPhone\" is interpreted to mean Microsoft Windows in this context, and we're looking for the company that created \"IPhone\" (interpreted as Microsoft Windows), we need to think about the company associated with the development and distribution of Microsoft Windows.\n","\n","In this context, the company that \"created\" Microsoft Windows would be Microsoft Corporation. Therefore, the answer to the puzzle, considering the given interpretation, would be Microsoft Corporation.\n","</p></blockquote>"],"metadata":{"id":"iVki6L8E6eeP"}},{"cell_type":"markdown","source":["Now, let's take a step back and overview different types of learning for NLP models:\n","\n","\n","\n","1. **Regular ML model training.** We collect a training set, then train the model from scratch on this training set. In the context of GPT, this is the **Pre-Training** stage, where an LLM is trained on a huge text dataset.\n","2. **Fine-tuning of a pre-trained model.** This is a model that was already trained on a general-purpose dataset, which is then tuned on smaller, task-specific data. ChatGPT is tuned to follow instructions during the **Supervised Fine-Tuning** stage and to align with human preferences during the **RLHF** stage.  \\\n","(The fine-tuning of the BERT model for sentiment analysis from the previous lesson falls into the same category.)\n","3. **In-context learning.** This refers to a situation where we give the model all the training data (usually this is very small) inside the prompt and want the model to extract patterns from the prompt. This type of learning doesn't involve any model training in the technical sense because the model does not change during this process.\n","\n","At the beginning of the ML revolution, only the first type of training was used; nowadays, all three of them are well established. When going from regular training, to fine-tuning, to context learning, we make more and more shallow changes of the model behaviour, but at the same time, it requires less and less data and computing power. This is very important: training a large model on terabytes of data for half a year is something only few companies can afford!"],"metadata":{"id":"NbuXTSIDxV2k"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=18Qzaoo5MKMZX_-wWCYuE4BhyRUgwO9d-\" width=\"600\" />\n","</center>"],"metadata":{"id":"weqJor7D9dO8"}},{"cell_type":"markdown","source":["# 4. ChatGPT as a General-Purpose AI\n","\n"],"metadata":{"id":"ohiEMbR5YqrR"}},{"cell_type":"markdown","source":["While it's easy to see that ChatGPT can give helpful answers to the most random questions, let's pose a different question: can it solve any of the  practical tasks that other ML applications can? It turns out that, because the core GPT model was trained on a very diverse dataset of texts, the ChatGPT model can indeed solve a lot of text-based tasks.\n","\n","Let's return to our task related to understanding the sentiments from user reviews for various businesses from the Yelp dataset that we previously worked with. As a refresher, let's load dataset and take a look:"],"metadata":{"id":"_dfb8MDKdsM3"}},{"cell_type":"code","source":["!pip install -qq datasets"],"metadata":{"id":"jaSvErKolhyl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6102fd38-3f2b-4f06-d78a-3ccce1d3259c","executionInfo":{"status":"ok","timestamp":1721053926067,"user_tz":-240,"elapsed":19353,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"yelp_review_full\")\n","dataset"],"metadata":{"id":"qS_w5O13llrO","colab":{"base_uri":"https://localhost:8080/","height":523,"referenced_widgets":["0ae3f959651c499ba5ff27e7cbd7890d","29a889435f174049ad75a52d39aa31a6","6c8150c7ffcc479bbf312ad7047b152b","46d0e1b00c4441c5bea95fc72033419b","5e46d3f85b314eedb117e4d3192fb093","f6453b6442a64d5aadd94f77eb14e4a7","b7cc6dc1cde94315ad519835a2c0133c","d844af07d88d42a8a3bd37913fb599d2","8b421f7e32d94b57b7c76a4977f83efa","be466339dddc4fd7b02a225f8fedd0a5","c749a1aa080e4b75b998c0710a30670f","65d20e6b89df49d7984eb6cc98ac854c","734b28c555034ebb90e01856ff608765","8a6be08fac4a40f6b8b9fe3076631e54","3823680de0194302ab5d9bde8321c580","17b1c9e230a947b786feadf7305bb916","17fb01d92b934e0a871fe59bf4b9299c","ad0bc8efd68c46cb9023557db12c1383","e04af19b26b343d89da489dfd888f032","2080613d13de42a7aa5ecd6e3e206520","118cb0c45dda49b283217603f8a8608f","6ee52a29805d461c9442a2e58ed9f8ad","61987a268f5048758aa93ad952da9714","dc4899de822b4a808474f956c5c6479a","dd86e6655884450db7afdfdedec3ca55","dd6552a961e34596a3ee63595dec137c","001f715b2a55458ba444a0b390b01c3c","ca4169c3c7b8415780f7a00ecd654ce0","67d72c6594434344b9f6b205cc67eab7","6ced759b1bc246b8a2efe8079409eb82","a12cb9d39496469fbff10822b0595300","34adfe08fd5e422e8767f9f4f86aa212","339cc3fa73ef4f328bddc7b47e8d9cc6","41faa9da165d48d7b5b66d4898869cf3","00b07826c9c144d2814dfaa75857acdc","9c4e9dc8130c449493e002590d02c525","05687535a63b47e28c5bd9a7c54695d2","1c662b24adbf4602948360d4ed8861d8","3f5960843af84182b90bc61a481f7e29","a7f68681920a420887201d604af39b88","f515bc15a8b345a3b3d689e5e782b4d0","86d0bce44aa246c2bdac0b0cfb8c227e","35396078a0a040048dba320b4ded8729","a4a4152138ac451bb02d6b8bb0d29de9","01387ae17e5d480abe5eed63d1ce0e5d","c23847c1b9b94baeb13a718a7eeebeb9","5bf0f79c823948c6bd7d54bf5b8348ef","7978e95b185a4b14a8f78bcdb1d8c53e","ed563b7e2c084ae88c892ba37cee3bc9","1fc94c570d4c4e958bbf9a1ab648fb50","03c1a3ac56654019a86658b241abd0e4","a2505cbbfaf047b7a1f38827784db24b","a84d9e58d9fd44478ddad427352b9685","91edfabee9584d8fafcd55d7dbf3caf8","20089f4e5d844e1589101cb9d673cec6"]},"executionInfo":{"status":"ok","timestamp":1721053946217,"user_tz":-240,"elapsed":20167,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"55ae600c-6679-4596-ddec-5ba97d35c155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae3f959651c499ba5ff27e7cbd7890d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/299M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d20e6b89df49d7984eb6cc98ac854c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/23.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61987a268f5048758aa93ad952da9714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41faa9da165d48d7b5b66d4898869cf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01387ae17e5d480abe5eed63d1ce0e5d"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['label', 'text'],\n","        num_rows: 650000\n","    })\n","    test: Dataset({\n","        features: ['label', 'text'],\n","        num_rows: 50000\n","    })\n","})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import pandas as pd\n","import textwrap\n","\n","some_samples = [0, 72, 167, 213, 316]\n","\n","for ind in some_samples:\n","    print(\"Review:\", textwrap.fill(dataset['train'][ind]['text']))\n","    print(\"Rating:\", int(dataset['train'][ind]['label']) + 1)\n","    print()"],"metadata":{"id":"2N9OHWhYlytt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053946217,"user_tz":-240,"elapsed":16,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"08872e02-5d03-4df2-9396-61c7b45f9d7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: dr. goldberg offers everything i look for in a general practitioner.\n","he's nice and easy to talk to without being patronizing; he's always\n","on time in seeing his patients; he's affiliated with a top-notch\n","hospital (nyu) which my parents have explained to me is very important\n","in case something happens and you need surgery; and you can get\n","referrals to see specialists without having to see him first.  really,\n","what more do you need?  i'm sitting here trying to think of any\n","complaints i have about him, but i'm really drawing a blank.\n","Rating: 5\n","\n","Review: Yay, I'm a fan but sometimes service is a little slow, it was very\n","good for us this visit.  Go to Papa j's every once in a while but\n","mostly for the White Pizza.  It is the best white pizza I have ever\n","had.  Order the white pizza on our visit this  weekend... it has\n","garlic, spinach, feta cheese and we usually add some veggie on top.\n","It was delicious!  Order fried calamari and it was OK...note to self\n","next time try the calamari roman style.\\n\\nLike the dinning room with\n","the hardwood floors and bright lighting.  \\n\\nThe bar was jumping thou\n","never go to the bar.\n","Rating: 4\n","\n","Review: Your typical area big box hardware store.  It's convenient to go to\n","because of the proximity to other stores and how close it is to so\n","many communities.\\n\\nCan't really say much else about this place other\n","than with the self checkout lanes it's a breeze to run in and then out\n","without having to deal with a single associate.\n","Rating: 3\n","\n","Review: I must admit, I do like the brews.  I also like the white cheddar mash\n","potatoes.  Other than that, meh.  The food is pretty forgettable.\n","Another \\\"brewery\\\" that sadly fails to be anything special in the\n","food department.  With the restaurants upping the ante on 8th Ave, you\n","may want to kick it up a notch Rock Bottom.  But how would anyone know\n","to?  It's a chain.  Whomp whomp!\n","Rating: 2\n","\n","Review: The worse \\\"Chinese food\\\", even the white rice which came with the\n","ogle sweet and sour chicken is really bad.  Please don't eat this\n","food.\n","Rating: 1\n","\n"]}]},{"cell_type":"markdown","source":["Now, in order to solve our sentiment analysis task – we’ll just ask ChatGPT to do it!"],"metadata":{"id":"CQD20E0_KIPX"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"dr. goldberg offers everything i look for in a general practitioner. he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first. really, what more do you need? i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"dvMwE3aSKHWn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053946875,"user_tz":-240,"elapsed":666,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"d969eb6c-c976-423f-9779-cdc33031d223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Based on the positive feedback provided in the review, it is likely\n","that this user rated the company with 5 stars.\n"]}]},{"cell_type":"markdown","source":["\n","And the answer is correct! Let's try another one with a rating of 1:"],"metadata":{"id":"WDYDnKoOLYNo"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"The worse \"Chinese food\", even the white rice which came with the ogle sweet and sour chicken is really bad. Please don't eat this food.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"iJcfS4q0KHYq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053948912,"user_tz":-240,"elapsed":2041,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"aa0e706f-c69a-4f56-c07e-431746adffe7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Based on the review, it seems like the user had a very negative\n","experience with the company. They specifically mentioned that the food\n","was bad and advised others not to eat it. Therefore, it is likely that\n","this user rated the company with 1 star out of 5.\n"]}]},{"cell_type":"markdown","source":["Also correct!\n","\n","Naturally, the next step will be to run an evaluation on the entire test dataset. However, we have a problem that needs to be addressed first: there is no uniform structure for the answers, meaning that it will be hard to extract and evaluate them with metrics. So, let’s try asking ChatGPT to format this properly:"],"metadata":{"id":"7t67Bhr1Lyih"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"dr. goldberg offers everything i look for in a general practitioner. he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first. really, what more do you need? i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","    At the end of your answer, give your final decision about the rating in the following format: \"Final Prediction: X stars\"\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"tTzAeARNKHat","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053949403,"user_tz":-240,"elapsed":500,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"daf78e01-9692-4714-ecc6-e894d6424d5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Based on the positive feedback provided in the review, it is likely\n","that the user rated this company 5 stars. Final Prediction: 5 stars\n"]}]},{"cell_type":"markdown","source":["This seems to be working, though we cannot be sure that the model will be consistent with it."],"metadata":{"id":"W0BNdTOUMR-v"}},{"cell_type":"markdown","source":["**Exercise.** Write a function that takes the text of a review as input and returns the star rating as an integer. If the answer is not in any recognizable format, the function should raise an exception."],"metadata":{"id":"WURtoXFa8_Sd"}},{"cell_type":"code","source":["# Exception class for failed parsing\n","class ReviewParseError(Exception):\n","    pass\n","\n","# passing also the OpenAI client as parameter for better usability\n","def predict_review_rating(text: str, client) -> int:\n","\n","    # <your code here>\n","\n","    pass"],"metadata":{"id":"QVkumwv19i8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <SOLUTION>\n","\n","import re\n","\n","class ReviewParseError(Exception):\n","    pass\n","\n","def predict_review_rating(text: str, client) -> int:\n","    prompt = textwrap.dedent(\"\"\"\n","            Here is a review of some company:\n","            \"{text}\"\n","            Predict how many stars from 1 to 5 this user rated this company.\n","            At the end of your answer, give your final decision about the\n","            rating in the following format: \"Final Prediction: X stars\".\n","        \"\"\").format(text=text)\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": prompt,}],\n","        temperature=0.0)\n","\n","    last_line = response.choices[0].message.content.split('\\n')[-1]\n","    m = re.match(r\"[\\w\\s]*[ :]+(?P<rating>[1-5])\\s?stars?[\\s.!,;]*\", last_line)\n","\n","    if m is None:\n","        raise ReviewParseError(\"Failed to parse the prediction.\")\n","    else:\n","        return int(m.group('rating'))"],"metadata":{"id":"qQo9Tgvuao4b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test out how it works:"],"metadata":{"id":"RBUTjjUZ-drW"}},{"cell_type":"code","source":["ind = 0\n","print(\"Review:\", textwrap.fill(dataset['train'][ind]['text']))\n","print(\"Rating:\", int(dataset['train'][ind]['label']) + 1)\n","print()\n","ans = predict_review_rating(dataset['train'][ind]['text'], client)\n","print(\"Answer:\", ans)"],"metadata":{"id":"PspGfFrLbKBQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721053951155,"user_tz":-240,"elapsed":1756,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"2d5c61f2-4712-4c98-f5ed-b3335f24d0b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: dr. goldberg offers everything i look for in a general practitioner.\n","he's nice and easy to talk to without being patronizing; he's always\n","on time in seeing his patients; he's affiliated with a top-notch\n","hospital (nyu) which my parents have explained to me is very important\n","in case something happens and you need surgery; and you can get\n","referrals to see specialists without having to see him first.  really,\n","what more do you need?  i'm sitting here trying to think of any\n","complaints i have about him, but i'm really drawing a blank.\n","Rating: 5\n","\n","Answer: 5\n"]}]},{"cell_type":"markdown","source":["**Exercise.** Evaluate this prediction function on the dataset. Determine both the accuracy and the percentage of parsing errors.\n","\n","_Note: This function will probably be very slow, but we don't want to bother too much with speed for now, so you can use around 100 examples for simplicity’s sake._\n","\n","<details>\n","    <summary><font color=\"404090\">Hint (click to expand).</font></summary>\n","\n","Setting the temperature to 0 may help a lot.\n","\n","</details>"],"metadata":{"id":"lEuKFYko-rFr"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"mw_FhhlOPFGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <SOLUTION>\n","\n","import random\n","from tqdm import tqdm\n","\n","def evaluate_chatgpt_predictions(dataset, client, subset_size=1, seed=12345):\n","    indexes = list(range(len(dataset)))\n","    r = random.Random(seed)\n","    r.shuffle(indexes)\n","    indexes = indexes[:subset_size]\n","\n","    correct = 0\n","    failed = 0\n","    for ind in tqdm(indexes):\n","        text = dataset[ind]['text']\n","        label = int(dataset[ind]['label']) + 1\n","        try:\n","            pred = predict_review_rating(text, client)\n","            if label == pred:\n","                correct += 1\n","        except ReviewParseError:\n","            failed += 1\n","\n","    return correct / len(indexes), failed / len(indexes)\n","\n","acc, fail = evaluate_chatgpt_predictions(dataset['test'], client, subset_size=100)\n","print()\n","print(\"Accuracy: \", acc)\n","print(\"Failed: \", fail)"],"metadata":{"id":"2a8EeCK6gqMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721054256868,"user_tz":-240,"elapsed":120132,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"1f8cb4f3-9326-437b-850a-e3072e5441d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [01:59<00:00,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  0.61\n","Failed:  0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["In our solution, we ended up with 65% accuracy and a 0% failure rate. However, since the ChatGPT is updated regularly, your results may vary. In any case, they are probably similar, and it’s quite impressive that this model has an accuracy level higher than the linear one, considering that there was no actual training performed, just a little bit of in-context learning."],"metadata":{"id":"w4OtDIiq6lDJ"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=18ltp1vYx6PYcohdOOZ2zbtiplXQcxfGP\" width=320 /></center>"],"metadata":{"id":"ON7qRGKvz8gb"}},{"cell_type":"markdown","source":["There is one thing worth mentioning however: this experiment, making predictions with out-of-the-box ChatGPT, may show an incorrect result. That is because the dataset could  have **leaked** into the GPT training data. We can't know this for certain, but given that the model was trained on a huge dataset of texts sourced from the Internet, (and given that Yelp dataset is not very novel), there’s a good chance that it ended up in the training data. The model hasn't been trained to predict the sentiment of these texts, but it may have seen the texts and their labels somewhere in close context.\n","\n","Potential test data leakage is actually a huge problem for benchmarking contemporary LLMs. Most of the state-of-the-art models are regularly updated with new data downloaded from the Internet so that they can answer prompts with more up-to-date information (and also just to get even more training data). So, if someone creates an open benchmark for testing LLMs, it will leak into the training data soon enough, and thus, invalidate all the testing. Unfortunately, as for now, there is no set protocol on how to do these things correctly."],"metadata":{"id":"o32SCHLDeOSK"}},{"cell_type":"markdown","source":["# 5. LangChain + OpenAI API\n"],"metadata":{"id":"aUIQ8Pf5YgxR"}},{"cell_type":"markdown","source":["## LangChain Basics"],"metadata":{"id":"DovnY0nYZGBP"}},{"cell_type":"markdown","source":["Now that we've tried writing applications on top of ChatGPT, let's learn how to do this using some even more powerful tools. **LangChain** is one of those tools, and it helps us build applications on top of language models. First, let's install it and set it up."],"metadata":{"id":"QbE-PUSrZPRW"}},{"cell_type":"code","source":["!pip install -qq langchain langchain-openai"],"metadata":{"id":"ay6uAs8ZsFYJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721054270460,"user_tz":-240,"elapsed":13611,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"6c4bc57e-9365-4403-eccc-ab9bbbc8a7bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Using this library, you can build what are called **chains**, that is, different steps for processing LLMs. Let's build a simple chain for our Yelp review prediction. When using the LangChain library our chain will contain 3 parts:\n","\n","1. A prompt template for converting input data (review) into the LLM prompt\n","2. An LLM interface for processing the prompt\n","3. An output parser for converting the LLMs output into a proper data type\n","\n","As you can see, this is very similar to what we've done before, just split into more standardized blocks.\n","\n","So, first, let's build a prompt template. And for better parsing quality, let's ask ChatGPT to output the results in JSON format, instead of plain text:"],"metadata":{"id":"KOgcieFXal3Y"}},{"cell_type":"code","source":["# just a general template for prompts\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# adding all our prompt text, plus a {review} template variable\n","json_prompt = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"Here is a review of some company:\n","\n","            \"{review}\"\n","\n","            Return a JSON object with a `rating` key that stores a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","        \"\"\")\n",")"],"metadata":{"id":"XrAlVJCXbxCR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, let's create an LLM, which is basically just ChatGPT wrapped in a LangChain class:"],"metadata":{"id":"8o--0czTcJ5N"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n","                 temperature=0.0,\n","                 openai_api_key=OPENAI_API_KEY)"],"metadata":{"id":"A7p8xPmraan1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we’ll import an output parser, which is just an out-of-the box feature in LangChain for parsing JSON:"],"metadata":{"id":"iW5wtJjGce8j"}},{"cell_type":"code","source":["from langchain.output_parsers.json import SimpleJsonOutputParser\n","\n","json_parser = SimpleJsonOutputParser()"],"metadata":{"id":"xkX0ClRWcpa0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, the last step – connecting all the elements into one chain using this neat syntax:"],"metadata":{"id":"GDoOHIRsctWp"}},{"cell_type":"code","source":["chain = json_prompt | llm | json_parser"],"metadata":{"id":"mH_b4vvU1Pqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test it! To run a chain, we call the function `invoke`, and we pass it our named parameters from the prompt:"],"metadata":{"id":"neUd4wh7c3ZY"}},{"cell_type":"code","source":["chain.invoke({\"review\": dataset['train'][0]['text']})"],"metadata":{"id":"wQyFbOtb0U5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721054271373,"user_tz":-240,"elapsed":429,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"5d1f269d-1439-41e8-eaac-2ba103991a36"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rating': 5}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["**Exercise.** Evaluate this new function and find out its accuracy and failure rate. Again, to save time you can just use 100 examples)."],"metadata":{"id":"swI6ZeV4P2Br"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"JTummTsnQGsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <SOLUTION>\n","\n","import random\n","from tqdm import tqdm\n","\n","def evaluate_chatgpt_predictions_2(dataset, chain, subset_size=1, seed=12345):\n","    indexes = list(range(len(dataset)))\n","    r = random.Random(seed)\n","    r.shuffle(indexes)\n","    indexes = indexes[:subset_size]\n","\n","    correct = 0\n","    failed = 0\n","    for ind in tqdm(indexes):\n","        text = dataset[ind]['text']\n","        label = int(dataset[ind]['label']) + 1\n","        ans = chain.invoke({\"review\": text})\n","        try:\n","            pred = int(ans['rating'])\n","            if label == pred:\n","                correct += 1\n","        except (KeyError, ValueError, TypeError):\n","            failed += 1\n","\n","    return correct / len(indexes), failed / len(indexes)"],"metadata":{"id":"5ORNOmFt5Hlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc, fail = evaluate_chatgpt_predictions_2(dataset['test'], chain, subset_size=100)\n","print()\n","print(\"Accuracy: \", acc)\n","print(\"Failed: \", fail)"],"metadata":{"id":"l8GcpWrXTb4e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721054328491,"user_tz":-240,"elapsed":56730,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"e9beb3bf-80b4-4824-9f81-86eb8e3a83c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:56<00:00,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  0.68\n","Failed:  0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["The accuracy was probably very similar to those we saw before, but with much less manual parsing needed.\n","\n","There is also a way to improve LLM prediction results by using a technique called **Chain of Thought(s)** (CoT). This is when we provide some additional text in the prompt that helps the model elaborate more on the answer before arriving at the final result. In this task with Yelp, it doesn't seem to help much, but we’ll demonstrate how to do it with LangChain anyway because it’s a useful tool.\n","\n","We’ll use the simplest version of CoT by adding the phrase \"Let's think step by step\" to our prompt. This will encourage the model to elaborate more, but it will also ruin the JSON format. So, we’ll need to pass the output again to the LLM and ask it to build the answer in JSON format; this is done by creating two chains and passing one to the other:"],"metadata":{"id":"B__X-ytrNdmn"}},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# first prompt with elaboration and prediction\n","prompt_1 = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"Here is a review of some company:\n","\n","            \"{review}\"\n","\n","            You need to analyse the sentiment of this text. Then make a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","            Let's think step by step.\n","        \"\"\")\n",")\n","\n","# chain with simple string output\n","chain_1 = prompt_1 | llm | StrOutputParser()\n","\n","# second prompt just for JSON extraction\n","prompt_2 = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"\n","            Here is the analysis of the sentiment of a user review:\n","\n","            \"{cot}\"\n","\n","            Return a JSON object with a `rating` key that stores a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","        \"\"\")\n",")\n","\n","# second chain using syntax for passing results from one chain to another\n","chain_2 = {\"cot\": chain_1} | prompt_2 | llm | json_parser"],"metadata":{"id":"03EnKpOTUCjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test how it works:"],"metadata":{"id":"4jorv8Pe9mZh"}},{"cell_type":"code","source":["chain_2.invoke({\"review\": dataset['train'][0]['text']})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ssubJ_hR5GL","executionInfo":{"status":"ok","timestamp":1721054330283,"user_tz":-240,"elapsed":1799,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"4f10c5bb-df24-4aa2-f72d-94fa75396dbb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rating': 5}"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["aacc, fail = evaluate_chatgpt_predictions_2(dataset['test'], chain_2, subset_size=100)\n","print()\n","print(\"Accuracy: \", acc)\n","print(\"Failed: \", fail)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLPZw4xJSgfX","executionInfo":{"status":"ok","timestamp":1721054610754,"user_tz":-240,"elapsed":280477,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"2b841895-7085-491c-b4c0-a08e4bd7ddb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [04:40<00:00,  2.81s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  0.68\n","Failed:  0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["**Bonus exercise.** Can you make use of this technique to actually improve the quality? Here’s [a guide on CoT](https://deepgram.com/learn/chain-of-thought-prompting-guide) that you can peruse if needed.\n","\n","_Note: as the change in quality decreases, you’ll need to use more examples for testing._"],"metadata":{"id":"ouQS6nbP7ibu"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1wkxdhtNHUm3S3aAbq4nK586mor8YIv5l\" width=320 /></center>"],"metadata":{"id":"p1CtPwPMXRdo"}},{"cell_type":"markdown","source":["## RAG with LangChain"],"metadata":{"id":"ITDcwTDJZAih"}},{"cell_type":"markdown","source":["Just to see how powerful LangChain is, let's implement another thing using it: **Retrieval Augmented Generation**, or **RAG**. The idea behind this approach is to improve the LLMs question-answering abilities by adding search results from a search engine or a database as additional information. This way, we utilize both the LLMs ability to generate intelligent answers, and we also make sure that the information it uses is as up-to-date as possible.\n","\n","A simple RAG in LangChain will work almost out-of-the-box, so let's make one, and get a general overview of how it works.\n","\n","We’ll need to know about two new concepts for this task: **agents** and **tools**. Essentially, an agent is a high-level system that uses an LLM together with prompt templates and various tools to solve complex tasks. Tools are things like web search, Python code, various APIs (also known as \"plugins\" in ChatGPT web UI).\n","\n","So, in our case, we’ll just need to create an agent, and provide it with a web search tool and the appropriate prompts. We’ll make use DuckDuckGo search and a pre-made prompt from LangChain Hub designed for these kinds of agents:"],"metadata":{"id":"1vLpP9bjeHzp"}},{"cell_type":"code","source":["!pip install -qq duckduckgo-search langchainhub langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyPuxnrj8Pr1","executionInfo":{"status":"ok","timestamp":1721054774601,"user_tz":-240,"elapsed":11966,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"22f797ea-9129-4473-cf1d-26bc0ba18d09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain.agents import AgentExecutor, create_react_agent\n","from langchain import hub\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","# just loading a prompt for ReAct agents (Reasoning + Acting)\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# using only one tool: web search\n","tools = [DuckDuckGoSearchRun()]\n","\n","# pass everything to an agent (llm was already defined before)\n","agent = create_react_agent(llm, tools, prompt)\n","# just another wrapper, but we are setting verbose to true to see the details\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"],"metadata":{"id":"_XVieslPfyhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's run the agent with a query that involves looking up some up-to-date information:"],"metadata":{"id":"L4UCSYd7BWrs"}},{"cell_type":"code","source":["agent_executor.invoke({\n","    \"input\": \"What is the most interesting scientific discovery of 2024?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik0GG-FLge8x","executionInfo":{"status":"ok","timestamp":1721054792339,"user_tz":-240,"elapsed":17743,"user":{"displayName":"Alexey Umnov","userId":"05815039600690784543"}},"outputId":"e4ed93ae-c6bc-4bdb-af80-ff62cbc720f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mI should use the duckduckgo_search tool to find the most recent scientific discoveries.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024\"\u001b[0m\u001b[36;1m\u001b[1;3m12 recent scientific breakthroughs. From cell reparation to reef restoration. 1. Restoring brain cells. Scientists in many fields received little recognition for the last couple of years, as the ... At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry ... At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical ... The long-anticipated Vera C. Rubin Observatory in Cerro Pachón, Chile, is scheduled to start observing on December 11, 2024. The facility will observe the Southern Hemisphere sky for a decade with a 3200-megapixel camera as part of the Legacy Survey of Space and Time. Each night will produce 20 terabytes of data. Discover the most exciting scientific breakthroughs that will shape the year 2024, from space exploration to biotechnology, in this informative and inspiring article by Popular Mechanics.\u001b[0m\u001b[32;1m\u001b[1;3mI need to narrow down my search to find the most interesting scientific discovery specifically.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 specific\"\u001b[0m\u001b[36;1m\u001b[1;3mAt CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical ... Discover the most exciting scientific breakthroughs that will shape the year 2024, from space exploration to biotechnology, in this informative and inspiring article by Popular Mechanics. Every year, we look for promising technologies poised to have a real impact on the world. Here are the advances that we think matter most right now. 12 recent scientific breakthroughs. From cell reparation to reef restoration. 1. Restoring brain cells. Scientists in many fields received little recognition for the last couple of years, as the ... From personalized medicine to lab automation to gene therapy, both established and emerging areas of innovation are set to help shape the future of the life sciences industry. Let's dive into ten of the biggest trending and emerging topics to watch in 2024. 1. Personalized medicine and cell therapies. Personalized medicine is a relatively new ...\u001b[0m\u001b[32;1m\u001b[1;3mI should try to find a specific article or source that highlights the most interesting scientific discovery of 2024.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 article\"\u001b[0m\u001b[36;1m\u001b[1;3m12 recent scientific breakthroughs. From cell reparation to reef restoration. 1. Restoring brain cells. Scientists in many fields received little recognition for the last couple of years, as the ... At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical ... The shift, which began last year, is suspected of helping make 2023 the hottest year in modern history, as temperatures in the first 11 months averaged more than 1.4°C above preindustrial levels—higher than rising emissions of greenhouse gases alone can explain. El Niño has suppressed the ocean's ability to absorb heat, and a drop in Sun ... The science events to watch for in 2024 Advanced AI tools, Moon missions and ultrafast supercomputers are among the developments set to shape research in the coming year. A piece of woolly mammoth skin excavated from permafrost has been found to contain fossilized chromosomes, making it possible to assemble the genome of extinct species, scientists say.\u001b[0m\u001b[32;1m\u001b[1;3mI should read the article that mentions the most interesting scientific discovery of 2024.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 article\"\u001b[0m\u001b[36;1m\u001b[1;3m3 Jan 2024. 4:00 PM ET. By Science News Staff. In October 2023, boats on Puraquequara Lake in Brazil were stranded after drought lowered the water level. BRUNO KELLY/REUTERS. Share: A version of this story appeared in Science, Vol 383, Issue 6678. Political upheaval and policy disputes could create uncertainties for scientists this year as a ... Discover the most exciting scientific breakthroughs that will shape the year 2024, from space exploration to biotechnology, in this informative and inspiring article by Popular Mechanics. The Spinoff Prize 2024. Nature introduces the finalists and those that made it on to the longlist for this year's award. Herb Brody. Outlook11 Jul 2024. Blowout! Satellites reveal one of the ... At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical ... Related article Ancient DNA reveals possible cause of mysterious population collapse 5,000 years ago, scientists say In prior findings, the fragments were also lacking an organized structure ...\u001b[0m\u001b[32;1m\u001b[1;3mI should look for a more specific article that directly mentions the most interesting scientific discovery of 2024.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 specific article\"\u001b[0m\u001b[36;1m\u001b[1;3mAcross disciplines and industries, scientific discoveries happen every day, so how can you stay ahead of emerging trends in a thriving landscape? At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more ... The shift, which began last year, is suspected of helping make 2023 the hottest year in modern history, as temperatures in the first 11 months averaged more than 1.4°C above preindustrial levels—higher than rising emissions of greenhouse gases alone can explain. El Niño has suppressed the ocean's ability to absorb heat, and a drop in Sun ... Discover the most exciting scientific breakthroughs that will shape the year 2024, from space exploration to biotechnology, in this informative and inspiring article by Popular Mechanics. The Spinoff Prize 2024. Nature introduces the finalists and those that made it on to the longlist for this year's award. Herb Brody. Outlook11 Jul 2024. Blowout! Satellites reveal one of the ... The science events to watch for in 2024 Advanced AI tools, Moon missions and ultrafast supercomputers are among the developments set to shape research in the coming year.\u001b[0m\u001b[32;1m\u001b[1;3mI should try to find a reputable source that specifically mentions the most interesting scientific discovery of 2024.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 reputable source\"\u001b[0m\u001b[36;1m\u001b[1;3mOver the past few years, there has been a transformation in how AI is used in scientific discoveries. From Deep Mind's AlphaFold - an AI system that accurately predicts the 3D models of protein structures - to discovering a new family of antibiotics and materials for more efficient batteries, the world is on the cusp of an AI-driven revolution in how new knowledge is discovered and used ... In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical validation of CRISPR, the rise of biomaterials, and the renewed progress in treating the undruggable, from cancer to neurodegenerative diseases. 12 recent scientific breakthroughs. From cell reparation to reef restoration. 1. Restoring brain cells. Scientists in many fields received little recognition for the last couple of years, as the ... And 2024 could be the year that scientists nail down the mass of the neutrino — the most mysterious particle in the standard model of particle physics. Results of the Karlsruhe Tritium Neutrino ... trusted source proofread ... New discoveries in gravitational waves (2024, February 26) ... Science X. The most comprehensive sci-tech news coverage on the web.\u001b[0m\u001b[32;1m\u001b[1;3mI should read the article from a reputable source that specifically mentions the most interesting scientific discovery of 2024.\n","Action: duckduckgo_search\n","Action Input: \"most interesting scientific discovery 2024 reputable source article\"\u001b[0m\u001b[36;1m\u001b[1;3mThe Top 10 Emerging Technologies of 2024 report highlights the technologies set to positively impact society within the next three to five years. At CAS, we have a unique view of recent scientific breakthroughs, the historical discoveries they were built upon, and the expertise to navigate the opportunities ahead. In 2023, we identified the top scientific breakthroughs, and 2024 has even more to offer. New trends to watch include the accelerated expansion of green chemistry, the clinical ... Breaking science news and articles on global warming, extrasolar planets, stem cells, bird flu, autism, nanotechnology, dinosaurs, evolution -- the latest discoveries ... A groundbreaking body of work led by Monash University physicists has opened a new pathway for understanding the universe's fundamental physics. Science News features daily news articles, feature stories, reviews and more in all disciplines of science, as well as Science News magazine archives back to 1924.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n","Final Answer: The most interesting scientific discovery of 2024 is the groundbreaking body of work led by Monash University physicists that has opened a new pathway for understanding the universe's fundamental physics.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input': 'What is the most interesting scientific discovery of 2024?',\n"," 'output': \"The most interesting scientific discovery of 2024 is the groundbreaking body of work led by Monash University physicists that has opened a new pathway for understanding the universe's fundamental physics.\"}"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["From the output you can see that the agent:\n","\n","\n","\n","1. Understood what it needs to search for, and created a search query\n","2. Got the search results from the search engine\n","3. Analyzed the results and generated the answer\n","\n","So, there’s a simple way to upgrade a search engine with LLMs, but even more complex RAG's are getting lots of action in many applications right now!"],"metadata":{"id":"MAMwqKM4Drq0"}},{"cell_type":"markdown","source":["# 6. Conclusion"],"metadata":{"id":"367f8e3QaDik"}},{"cell_type":"markdown","source":["In this lesson, we talked a lot about ChatGPT and its applications, so let's wrap up what we've learned:\n","\n","\n","\n","* We understood how the GPT model is built using **transformer** architecture and how ChatGPT is built on top of it\n","* We learned about different kinds of model learning, and most interestingly for LLMs, **in-context learning**\n","* We practised writing applications on top of LLMs, using both a basic **API** and the **LangChain** library\n","\n","Up until this point, we’ve mostly been talking about NLP tasks, but in the [**next lesson**](https://colab.research.google.com/github/Nebius-Academy/ML-Starter-Pack/blob/main/notebooks/lesson_5.ipynb), we’ll switch to a new kind of task: **image classification**."],"metadata":{"id":"sKz0ca90aG4I"}},{"cell_type":"markdown","source":["By the way, if you’re interested, here are some additional resources on the things we talked about in this lesson:\n","\n","* This [blog post from OpenAI](https://openai.com/blog/chatgpt) describes the ChatGPT model\n","* See a [GPT-2 implementation](https://jaykmody.com/blog/gpt-from-scratch/) from scratch in NumPy\n","* A [guide](https://deepgram.com/learn/chain-of-thought-prompting-guide) on the Chain-of-Thought technique\n","* A [simple guide](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain) on building RAGs for more specific applications than web search"],"metadata":{"id":"wP7uohwtWK4a"}}]}
