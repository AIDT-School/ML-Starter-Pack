{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1-UrUldqGx2Ezp8fqw0J5fOU2YXswfjsb\" width=640 /></center>\n","\n","<br/>\n","\n","<font size=6>\n","<center>Lesson 4: ChatGPT\n","</center>\n","</font>\n","\n","<font size=4>\n","<center><i>Discovering the many things that one model can do</i>\n","</center>\n","</font>\n","\n","\n"],"metadata":{"id":"oBVShEbLkQlx"}},{"cell_type":"markdown","source":["# 1. Introduction\n"],"metadata":{"id":"kYKZPaMmWUV_"}},{"cell_type":"markdown","source":["## Lesson Overview"],"metadata":{"id":"aNKTHB8srO6E"}},{"cell_type":"markdown","source":["This time, we’ll go deep into a machine learning model that’s made a lot of noise recently: ChatGPT. Fortunately, just like BERT from the previous lesson, it’s based on the **transformers** architecture, so it will be easier to understand what’s happening. In this lesson, you’ll:\n","\n","* Understand the structure of the ChatGPT model and how it’s **trained**\n","* Learn about **in-context learning** and how it relates to other types of learning\n","* Practice solving real-life problems using the **OpenAI API** and **LangChain** library\n","\n","You can find the notebook with solutions [here](https://colab.research.google.com/drive/1FkzEuJIjkJLXUFCMCpWpYbx1a7RkYwkP?usp=sharing)."],"metadata":{"id":"coG_sboIrWmS"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1djg_MFGfQwg68uPN4cKW5pt3YNMsGlmX\" width=1000 /></center>"],"metadata":{"id":"M5YuOoyrZcOD"}},{"cell_type":"markdown","source":["# 2. Understanding ChatGPT\n"],"metadata":{"id":"ZlMq_Ia_XVb3"}},{"cell_type":"markdown","source":["## The Base GPT model"],"metadata":{"id":"8D5UbFit2xO6"}},{"cell_type":"markdown","source":["First, a quick review of the **transformers** architecture from the previous lesson:"],"metadata":{"id":"wfugvp2xZ1M_"}},{"cell_type":"markdown","source":["\n","<center><img src=\"https://drive.google.com/uc?export=view&id=19D0hMWMKa2-wqt-6GN-RtoDwxvmb2AD-\" width=700 /></center>"],"metadata":{"id":"0vIKCExMrgVZ"}},{"cell_type":"markdown","source":["There are two major parts here:\n","\n","* An **encoder**, which takes a sequence of input tokens and generates a sequence of embedding vectors\n","* A **decoder**, which takes both the embedding vectors and the previous output tokens and generates next output tokens\n","\n","Note that there are three different types of embeddings: input token embeddings, encoder outputs, and output token embeddings. Token embeddings are connected to natural language, but encoder outputs are just vectors that only the model understands.\n","\n","In the previous lesson, we worked with a BERT model, which only has the encoder part; it builds the embedding vectors that can be used later for classifying the whole sequence, (for example, to detect the sentiments of the reviews).\n","\n","But what if instead, we build a model with only the decoder?\n","\n","This may sound a little strange – because how would we decode something without encoding it first? However, this way we actually arrive at a **generative model**. Rather than transforming an input sequence into an output sequence (like in a machine translation task), a generative model just continues its input sequence; it can even create a text from scratch. (We talked a little bit about models like this in lesson 1.)\n","\n","The following picture illustrates how it works:"],"metadata":{"id":"IoI75h5pdlD1"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=1x3dtwbvP1tbL7Jqys8QX_NW6AehJekPw\" width=600 />\n","</center>\n","\n","\n"],"metadata":{"id":"-ScU2J-Kd8C-"}},{"cell_type":"markdown","source":["A decoder-only model predicts the continuation of a text token by token, adding each new token to its input.\n","\n","Recent decoder-only models have billions of trainable parameters, and so they’re referred to as **Large Language Models** (**LLMs**).\n","\n","The emergence of LLMs was a huge breakthrough in machine learning, and these days, several Large Language Models appear every week. Let’s just mention a few of them:\n","\n","\n","\n","1. The **GPT** family, created by OpenAI, consists of decoder-only models (GPT means **Generative Pre-trained Transformer**). It includes very famous models like GPT-3.5, GPT-4 and GPT-4o\n","2. The **Claude** family by Anthropic\n","3. The **Gemini** family by Google\n","4. **Mistral** and **Mixtral**, which are open source and can be downloaded from Hugging Face.\n","\n","Let’s discuss the typical architecture for these models:\n","\n","\n","\n","* The input data is transformed into vectors using **token embeddings** and **positional embeddings**.\n","* These vectors go through a lot of big **transformer layers**. (Like, really huge layers with a ton of parameters.)\n","* The final vector embeddings are translated back into token probabilities using **softmax**.\n","\n","The training process for these models often consists of several steps (more details ahead!), but the first step is always the same: **pre-training**. During this step, the model browses through huge volumes of real texts and learns to reproduce them. Roughly speaking, for each text and for each token position of this text the model learns to solve a classification task: predicting the next token.\n","\n","After the pre-training stage we get the **base GPT model**."],"metadata":{"id":"bnlmtGr_s3Q3"}},{"cell_type":"markdown","source":["## From GPT to ChatGPT"],"metadata":{"id":"-VFFZmM3DQ6t"}},{"cell_type":"markdown","source":["Now, from the base GPT model, there are two more training steps to get the ChatGPT model."],"metadata":{"id":"PQjlJCX9SUHS"}},{"cell_type":"markdown","source":["**1. Supervised Fine-Tuning**\n","\n","The ability to continue a sentence is not enough – ChatGPT should complete tasks given by users and have the ability to keep up a conversation.\n","\n","To do that, OpenAI hired people who created specific fine tuning data: prompts paired with completions, this is where a prompt is given as a task and a completion solves this task. This could be something like this:\n","\n","<br/>\n","\n","| Prompt | Completion |\n","| -------- | -------- |\n","| How can I train an LLM?   | You need to start with pre-training and then...   |\n","| I feel depressed :(   | What's wrong? How can I help you?   |\n","\n","<br/>\n","\n","This step is just another supervised training step where the value comes from specific, high quality data."],"metadata":{"id":"PbndHlAJSeyk"}},{"cell_type":"markdown","source":["**2. Alignment training**\n","\n","We also expect an LLM not only to produce likely texts, but to be helpful, honest, and, well, “harmless”.\n","\n","Helpfulness can be achieved after Supervised Fine-Tuning, but after that the model can still produce harmful or toxic answers. (And we wouldn’t want ChatGPT to tell us how to assemble a bomb or to write an 18+ movie script, would we?)\n","\n","In other words, we need to ensure that the model aligns with human preferences. There are several ways of doing this; OpenAI used **RLHF** (**Reinforcement Learning on Human Feedback**).\n","\n","We won't go too far into the details about this type of learning, let’s just explain the basics.\n","\n","First, a **reward model** is trained. This is a model that is able to rank different completions of a prompt by assigning each a numerical score indicating how acceptable it is. So, a really toxic completion will receive a very low score, while a helpful, harmless one will get a much larger score.\n","\n","The reward model is trained on triplets (prompt, better completion, worse completion) labeled by human contractors.\n","\n","Next, RLHF trains the model to generate completions, maximizing the reward model score."],"metadata":{"id":"SxUUkQx7s0Ks"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=1oO3mzSFK3UrrCmNiqOSsRdfqfPKtJ7xt\" width=800 />\n","\n","*Image from [InstructGPT paper](https://arxiv.org/abs/2203.02155)*\n","</center>"],"metadata":{"id":"VFy4zwEf8Kfn"}},{"cell_type":"markdown","source":["This whole thing is the process of fine-tuning a general GPT model to create a ChatGPT model with more specific goals. We’ll continue overviewing this terminology a little bit later in the lesson.\n","\n","Let's now pause and get a zoomed-out view on ChatGPT, because it’s easy to get lost in all the complex details. In a nutshell, this model is similar to the sentiment prediction linear model from lesson 2; this is a machine learning model with parameters that are trained on some data with a specific goal. ChatGPT obviously seems much more intelligent than a simple linear model, and the magic happens because ChatGPT has more parameters and a bigger training dataset – both larger by several magnitudes. The numbers on the right are for ChatGPT, based on GPT-3.5."],"metadata":{"id":"MLqzU5xG8PEV"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1Ax621tRQzJsT44tqRL_EQ6QyxpS0Q4sW\" width=700 /></center>"],"metadata":{"id":"5Ia7nTzHiKke"}},{"cell_type":"markdown","source":["Note that because ChatGPT is still a machine learning model, there can be some unexpected effects to this, for example:\n","\n","* The model does not actually give the best possible answers for the prompts, it instead tries to give the answer that was more suitable during training. The difference is subtle, but there are some known cases of “weird” behaviour. For example, you can add things like \"take a deep breath before answering\", or, \"I'll tip you $100 if you answer well\", to the prompt, and the results will improve. It's not entirely clear why the model does this, but this could be the result of seeing that humans answer better after those prompts.\n","* Even after Supervised Fine-Tuning and RLHF, the model can still make mistakes or **hallucinate** – in other words, create something entirely new. (For example, if you ask it to name several research papers about elven artifacts of Gondolin, it will probably make up some just to be helpful and nice.)"],"metadata":{"id":"Khd0OOGCkAFe"}},{"cell_type":"markdown","source":["## OpenAI API"],"metadata":{"id":"hLwj8Ufz2qWm"}},{"cell_type":"markdown","source":["Now, you’ve probably already tried out the ChatGPT in action (if not, we highly recommend doing so by visiting [https://chat.openai.com/](https://chat.openai.com/)). Now, we want to show you another way to use it: through OpenAI API, which is a more flexible endpoint for the same thing. Let's try it out, and we’ll also compare how the answers from the base GPT model and ChatGPT model differ.\n","\n","First, we need to install and import the “openai” package which is the wrapper for the API in Python."],"metadata":{"id":"YyP2OIzpntEG"}},{"cell_type":"code","source":["!pip install -qq openai"],"metadata":{"id":"HQp8BOhjhAfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai"],"metadata":{"id":"8p9h3atchORt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to make requests to this API, we need to give it a proper API key so that it knows the account the requests should be attributed to, and charged from. (If you’ve never used the API platform before, you’ll get some free credits for the start that should be sufficient for our lessons.)"],"metadata":{"id":"-M_ZmWc3osiL"}},{"cell_type":"code","source":["OPENAI_API_KEY = \"<insert your key here>\"\n","\n","client = openai.OpenAI(api_key=OPENAI_API_KEY)"],"metadata":{"id":"ZThVn_k1hqN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Now, let’s give ChatGPT a prompt and check out its answer. With the API, you need to explicitly state the model you want to use;  we’ll be using ChatGPT-3.5 Turbo because it’s much cheaper than the more recent GPT-4o, yet it still provides good answers. Here’s the code for that (see the comments for explanations):"],"metadata":{"id":"dSnXx5FVpGDd"}},{"cell_type":"code","source":["response = client.chat.completions.create(   # this function is for chat models\n","    model=\"gpt-3.5-turbo\",   # this is the name of the model in the system\n","    messages=[   # prompts are given in a structured format of messages\n","        {\n","            \"role\": \"user\",   # this means that the message is from user\n","            \"content\": \"Write a haiku\",   # the actual text of the prompt\n","        }\n","    ]\n",")\n","response"],"metadata":{"id":"rxUtEvXTsbca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The response object contains lots of meta-information, but for our purposes just the text is enough:"],"metadata":{"id":"3BfDGQ-ctJKf"}},{"cell_type":"code","source":["print(response.choices[0].message.content)"],"metadata":{"id":"XSNkOnvttIJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As previously discussed, the ChatGPT model is built on top of a more general GPT model. The only base model available is the one for the GPT-3 model, because there is not much point in using those. But we can get access to that via the API and analyse its responses for a better understanding of ChatGPT:"],"metadata":{"id":"_EWWl0JYuZA6"}},{"cell_type":"code","source":["# this model uses a different, more simple legacy API\n","response = client.completions.create(\n","    model=\"davinci-002\",    # the name of the base GPT-3 model\n","    prompt=\"Write a haiku\")\n","print(response.choices[0].text)"],"metadata":{"id":"HtiXtFWiu-xF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, the answer is very strange. This is because this model never learned to follow the instructions; rather, it just tries to continue text generation from a given start instead. From this point of view, the answer makes more sense.\n","\n","Note that if we phrase the prompt as if we are only waiting for completion, both models will do well with it:"],"metadata":{"id":"zfIl84UouZLF"}},{"cell_type":"code","source":["response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[\n","        {\"role\": \"user\", \"content\": \"From the moment I walked in, I\"}\n","    ],\n","    max_tokens=200,   # let's limit the number of tokens to prevent generation of very long answers\n",")\n","print(response.choices[0].message.content)"],"metadata":{"id":"EZqtDbASv0Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = client.completions.create(\n","    model=\"davinci-002\",\n","    prompt=\"From the moment I walked in, I\",\n","    max_tokens=200,\n",")\n","print(response.choices[0].text)"],"metadata":{"id":"UmoINDnTv6p-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's do a small exercise to get more familiar with the API.\n","\n","There is a parameter in the API called `temperature` that controls the randomness of the answers. Do you recall that the model generates one output token at a time, and each time predicts the probability of each possible next token? Temperature controls how the actual generated token is chosen:\n","\n","\n","\n","* If the temperature is 0, the model just takes the token with the highest probability\n","* With smaller temperatures, the model generates a random token, but with a very high preference for high probability tokens (it amplifies their already high probabilities)\n","* Then, with higher temperatures, the model starts generating more low probability tokens, at some point starting to boost lower probability tokens to make them more even with others\n","\n","In the OpenAI API, this is described as a balance between consistency (low temperatures) and diversity or creativity (high temperatures)."],"metadata":{"id":"R01bOLoB6TaN"}},{"cell_type":"markdown","source":["**Exercise.** Try using different temperature values (it has a range from 0 to 2) with a single prompt and analyse how the results change. Use the parameter _n_ to generate several answers with one request."],"metadata":{"id":"XrWzIzrl6YPc"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"DMCRAbuPKtxB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well, it seems the maximum temperature results are totally out of control (creative), but this is an expected behaviour."],"metadata":{"id":"k2gq_EwIGHfk"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1Qtbbu3FicxKTVXERSFIiV5-PkjxRk4kN\" width=256 /></center>"],"metadata":{"id":"DfwNI6bugfK2"}},{"cell_type":"markdown","source":["If you want to play more with this sampling aspect, there’s another parameter called `top_p` that can cut-off lowest probability tokens (you can read about it in [API documentation](https://platform.openai.com/docs/api-reference/chat/create)).\n","\n","In the following sections, we’ll explore the capabilities of this model and see what applications can be built on top of it."],"metadata":{"id":"iSafb3j_rF1q"}},{"cell_type":"markdown","source":["# 3. In-context Learning"],"metadata":{"id":"yHTsScvuXrXP"}},{"cell_type":"markdown","source":["Before starting to build applications on top of ChatGPT, let's understand one important concept relevant to it: **in-context learning** (or **few-shot learning)**. This is the ability to temporarily learn new things from the data provided in the prompt. Let's see some examples; below is a typical question that pulls the information from all the base knowledge of GPT (with the addition of nice phrasing trained in ChatGPT):\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","What company created the IPhone?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The iPhone was created by Apple Inc. Apple is a technology company that designs, manufactures, and markets consumer electronics, computer software, and other products, including the iPhone. The first iPhone was introduced by Apple's co-founder, Steve Jobs, and it was initially released on June 29, 2007. Since then, Apple has continued to release new models of the iPhone, making it one of the most popular and widely used smartphones globally.\n","</p></blockquote>\n","\n","\n"],"metadata":{"id":"_gqGzB8-sCLx"}},{"cell_type":"markdown","source":["Now, let's make it harder and make the question a bit cryptic.\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","In this text hgieugheda means IPhone. What company created hgieugheda?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The term \"hgieugheda\" does not correspond to any recognizable product or brand name, and as of my last knowledge update in January 2022, it is not associated with the iPhone or any known technology product. If \"hgieugheda\" is a fictional term or code you've created to represent the iPhone in a specific context, it's important to note that in reality, the iPhone is a product of Apple Inc. If there have been developments or changes after my last update, I would not be aware of them.\n","</p></blockquote>\n","\n","Despite adding an unnecessary critique of our prompt, the model still managed to give the correct answer to our question. This is a primitive example of in-context learning: the model learned new \"rules\" of the prompt and answered accordingly."],"metadata":{"id":"13Laer5Y4Lp4"}},{"cell_type":"markdown","source":["We can make it even more explicit by giving training examples, and asking for predictions. Let's ask it to classify some food between fruits and vegetables, but without telling the model what the rule is, just assigning values 0 (vegetable) and 1 (fruit) to them:\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","tomato - 0\n","\n","orange - 1\n","\n","apple - 1\n","\n","cucumber - 0\n","\n","bellpepper - 0\n","\n","Now following the same rule, answer this:\n","\n","pear - ?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The pattern in the provided list appears to be assigning a value of 1 to fruits and a value of 0 to vegetables. Based on this pattern:\n","\n","pear - 1\n","\n","So, according to the established rule, the value for \"pear\" would be 1.\n","</p></blockquote>\n","\n","This is a simple example, but anyway we can treat this as learning and predicting."],"metadata":{"id":"NE_qH91z7mZ3"}},{"cell_type":"markdown","source":["However, there is a limit to how far we can take this:\n","\n","**👤 Prompt:**\n","\n","<blockquote><p>\n","In this text, iPhone means Microsoft Windows. What company created the iPhone?\n","</p></blockquote>\n","\n","**🤖 ChatGPT:**\n","\n","<blockquote><p>\n","The iPhone was created by Apple Inc., not Microsoft Windows. Apple is the company responsible for designing and manufacturing the iPhone, which is a line of smartphones that run on the iOS operating system developed by Apple. Microsoft Windows, on the other hand, is an operating system developed by Microsoft for personal computers, not mobile devices like the iPhone. If your text is using \"iPhone\" to refer to Microsoft Windows, it may be a misrepresentation or a mistake, as these are products of different companies.\n","</p></blockquote>\n","\n","We can see that the model tried really hard to give the correct answer, but the \"memory\" from training data went into conflict with the context, and the \"memory\" prevailed. This once again shows us the limits of what ChatGPT is capable of: it could be hard for it to generate an unlikely answer like \"Microsoft created IPhone\", even though in the given context this is the correct answer.\n"],"metadata":{"id":"Qr2vSHUu7hiH"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=114PQPyQ7fUSmG8n3aUwb1m8APdTz-wTS\" width=320 /></center>"],"metadata":{"id":"6bNMApxSOLtl"}},{"cell_type":"markdown","source":["**Exercise.** Can you engineer a prompt that will help the model answer this question correctly? Try not to change the question, just add instructions about how to approach it."],"metadata":{"id":"LLpyNy4u5zs-"}},{"cell_type":"markdown","source":["Now, let's take a step back and overview different types of learning for NLP models:\n","\n","\n","\n","1. **Regular ML model training.** We collect a training set, then train the model from scratch on this training set. In the context of GPT, this is the **Pre-Training** stage, where an LLM is trained on a huge text dataset.\n","2. **Fine-tuning of a pre-trained model.** This is a model that was already trained on a general-purpose dataset, which is then tuned on smaller, task-specific data. ChatGPT is tuned to follow instructions during the **Supervised Fine-Tuning** stage and to align with human preferences during the **RLHF** stage.  \\\n","(The fine-tuning of the BERT model for sentiment analysis from the previous lesson falls into the same category.)\n","3. **In-context learning.** This refers to a situation where we give the model all the training data (usually this is very small) inside the prompt and want the model to extract patterns from the prompt. This type of learning doesn't involve any model training in the technical sense because the model does not change during this process.\n","\n","At the beginning of the ML revolution, only the first type of training was used; nowadays, all three of them are well established. When going from regular training, to fine-tuning, to context learning, we make more and more shallow changes of the model behaviour, but at the same time, it requires less and less data and computing power. This is very important: training a large model on terabytes of data for half a year is something only few companies can afford!"],"metadata":{"id":"NbuXTSIDxV2k"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=18Qzaoo5MKMZX_-wWCYuE4BhyRUgwO9d-\" width=\"600\" />\n","</center>"],"metadata":{"id":"weqJor7D9dO8"}},{"cell_type":"markdown","source":["# 4. ChatGPT as a General-Purpose AI\n","\n"],"metadata":{"id":"ohiEMbR5YqrR"}},{"cell_type":"markdown","source":["While it's easy to see that ChatGPT can give helpful answers to the most random questions, let's pose a different question: can it solve any of the  practical tasks that other ML applications can? It turns out that, because the core GPT model was trained on a very diverse dataset of texts, the ChatGPT model can indeed solve a lot of text-based tasks.\n","\n","Let's return to our task related to understanding the sentiments from user reviews for various businesses from the Yelp dataset that we previously worked with. As a refresher, let's load dataset and take a look:"],"metadata":{"id":"_dfb8MDKdsM3"}},{"cell_type":"code","source":["!pip install -qq datasets"],"metadata":{"id":"jaSvErKolhyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"yelp_review_full\")\n","dataset"],"metadata":{"id":"qS_w5O13llrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import textwrap\n","\n","some_samples = [0, 72, 167, 213, 316]\n","\n","for ind in some_samples:\n","    print(\"Review:\", textwrap.fill(dataset['train'][ind]['text']))\n","    print(\"Rating:\", int(dataset['train'][ind]['label']) + 1)\n","    print()"],"metadata":{"id":"2N9OHWhYlytt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, in order to solve our sentiment analysis task – we’ll just ask ChatGPT to do it!"],"metadata":{"id":"CQD20E0_KIPX"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"dr. goldberg offers everything i look for in a general practitioner. he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first. really, what more do you need? i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"dvMwE3aSKHWn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","And the answer is correct! Let's try another one with a rating of 1:"],"metadata":{"id":"WDYDnKoOLYNo"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"The worse \"Chinese food\", even the white rice which came with the ogle sweet and sour chicken is really bad. Please don't eat this food.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"iJcfS4q0KHYq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also correct!\n","\n","Naturally, the next step will be to run an evaluation on the entire test dataset. However, we have a problem that needs to be addressed first: there is no uniform structure for the answers, meaning that it will be hard to extract and evaluate them with metrics. So, let’s try asking ChatGPT to format this properly:"],"metadata":{"id":"7t67Bhr1Lyih"}},{"cell_type":"code","source":["# remove unnecessary whitespaces using dedent function\n","prompt = textwrap.dedent(\"\"\"\n","    Here is a review of some company:\n","\n","    \"dr. goldberg offers everything i look for in a general practitioner. he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first. really, what more do you need? i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"\n","\n","    Predict how many stars from 1 to 5 this user rated this company.\n","    At the end of your answer, give your final decision about the rating in the following format: \"Final Prediction: X stars\"\n","\"\"\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature = 0.0)\n","\n","# use fill function for better readability\n","print(textwrap.fill(response.choices[0].message.content))"],"metadata":{"id":"tTzAeARNKHat"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This seems to be working, though we cannot be sure that the model will be consistent with it."],"metadata":{"id":"W0BNdTOUMR-v"}},{"cell_type":"markdown","source":["**Exercise.** Write a function that takes the text of a review as input and returns the star rating as an integer. If the answer is not in any recognizable format, the function should raise an exception."],"metadata":{"id":"WURtoXFa8_Sd"}},{"cell_type":"code","source":["# Exception class for failed parsing\n","class ReviewParseError(Exception):\n","    pass\n","\n","# passing also the OpenAI client as parameter for better usability\n","def predict_review_rating(text: str, client) -> int:\n","\n","    # <your code here>\n","\n","    pass"],"metadata":{"id":"QVkumwv19i8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test out how it works:"],"metadata":{"id":"RBUTjjUZ-drW"}},{"cell_type":"code","source":["ind = 0\n","print(\"Review:\", textwrap.fill(dataset['train'][ind]['text']))\n","print(\"Rating:\", int(dataset['train'][ind]['label']) + 1)\n","print()\n","ans = predict_review_rating(dataset['train'][ind]['text'], client)\n","print(\"Answer:\", ans)"],"metadata":{"id":"PspGfFrLbKBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise.** Evaluate this prediction function on the dataset. Determine both the accuracy and the percentage of parsing errors.\n","\n","_Note: This function will probably be very slow, but we don't want to bother too much with speed for now, so you can use around 100 examples for simplicity’s sake._\n","\n","<details>\n","    <summary><font color=\"404090\">Hint (click to expand).</font></summary>\n","\n","Setting the temperature to 0 may help a lot.\n","\n","</details>"],"metadata":{"id":"lEuKFYko-rFr"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"mw_FhhlOPFGv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In our solution, we ended up with 65% accuracy and a 0% failure rate. However, since the ChatGPT is updated regularly, your results may vary. In any case, they are probably similar, and it’s quite impressive that this model has an accuracy level higher than the linear one, considering that there was no actual training performed, just a little bit of in-context learning."],"metadata":{"id":"w4OtDIiq6lDJ"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=18ltp1vYx6PYcohdOOZ2zbtiplXQcxfGP\" width=320 /></center>"],"metadata":{"id":"ON7qRGKvz8gb"}},{"cell_type":"markdown","source":["There is one thing worth mentioning however: this experiment, making predictions with out-of-the-box ChatGPT, may show an incorrect result. That is because the dataset could  have **leaked** into the GPT training data. We can't know this for certain, but given that the model was trained on a huge dataset of texts sourced from the Internet, (and given that Yelp dataset is not very novel), there’s a good chance that it ended up in the training data. The model hasn't been trained to predict the sentiment of these texts, but it may have seen the texts and their labels somewhere in close context.\n","\n","Potential test data leakage is actually a huge problem for benchmarking contemporary LLMs. Most of the state-of-the-art models are regularly updated with new data downloaded from the Internet so that they can answer prompts with more up-to-date information (and also just to get even more training data). So, if someone creates an open benchmark for testing LLMs, it will leak into the training data soon enough, and thus, invalidate all the testing. Unfortunately, as for now, there is no set protocol on how to do these things correctly."],"metadata":{"id":"o32SCHLDeOSK"}},{"cell_type":"markdown","source":["# 5. LangChain + OpenAI API\n"],"metadata":{"id":"aUIQ8Pf5YgxR"}},{"cell_type":"markdown","source":["## LangChain Basics"],"metadata":{"id":"DovnY0nYZGBP"}},{"cell_type":"markdown","source":["Now that we've tried writing applications on top of ChatGPT, let's learn how to do this using some even more powerful tools. **LangChain** is one of those tools, and it helps us build applications on top of language models. First, let's install it and set it up."],"metadata":{"id":"QbE-PUSrZPRW"}},{"cell_type":"code","source":["!pip install -qq langchain langchain-openai"],"metadata":{"id":"ay6uAs8ZsFYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using this library, you can build what are called **chains**, that is, different steps for processing LLMs. Let's build a simple chain for our Yelp review prediction. When using the LangChain library our chain will contain 3 parts:\n","\n","1. A prompt template for converting input data (review) into the LLM prompt\n","2. An LLM interface for processing the prompt\n","3. An output parser for converting the LLMs output into a proper data type\n","\n","As you can see, this is very similar to what we've done before, just split into more standardized blocks.\n","\n","So, first, let's build a prompt template. And for better parsing quality, let's ask ChatGPT to output the results in JSON format, instead of plain text:"],"metadata":{"id":"KOgcieFXal3Y"}},{"cell_type":"code","source":["# just a general template for prompts\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# adding all our prompt text, plus a {review} template variable\n","json_prompt = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"Here is a review of some company:\n","\n","            \"{review}\"\n","\n","            Return a JSON object with a `rating` key that stores a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","        \"\"\")\n",")"],"metadata":{"id":"XrAlVJCXbxCR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, let's create an LLM, which is basically just ChatGPT wrapped in a LangChain class:"],"metadata":{"id":"8o--0czTcJ5N"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n","                 temperature=0.0,\n","                 openai_api_key=OPENAI_API_KEY)"],"metadata":{"id":"A7p8xPmraan1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we’ll import an output parser, which is just an out-of-the box feature in LangChain for parsing JSON:"],"metadata":{"id":"iW5wtJjGce8j"}},{"cell_type":"code","source":["from langchain.output_parsers.json import SimpleJsonOutputParser\n","\n","json_parser = SimpleJsonOutputParser()"],"metadata":{"id":"xkX0ClRWcpa0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, the last step – connecting all the elements into one chain using this neat syntax:"],"metadata":{"id":"GDoOHIRsctWp"}},{"cell_type":"code","source":["chain = json_prompt | llm | json_parser"],"metadata":{"id":"mH_b4vvU1Pqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test it! To run a chain, we call the function `invoke`, and we pass it our named parameters from the prompt:"],"metadata":{"id":"neUd4wh7c3ZY"}},{"cell_type":"code","source":["chain.invoke({\"review\": dataset['train'][0]['text']})"],"metadata":{"id":"wQyFbOtb0U5J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise.** Evaluate this new function and find out its accuracy and failure rate. Again, to save time you can just use 100 examples)."],"metadata":{"id":"swI6ZeV4P2Br"}},{"cell_type":"code","source":["# <your code here>"],"metadata":{"id":"JTummTsnQGsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc, fail = evaluate_chatgpt_predictions_2(dataset['test'], chain, subset_size=100)\n","print()\n","print(\"Accuracy: \", acc)\n","print(\"Failed: \", fail)"],"metadata":{"id":"l8GcpWrXTb4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The accuracy was probably very similar to those we saw before, but with much less manual parsing needed.\n","\n","There is also a way to improve LLM prediction results by using a technique called **Chain of Thought(s)** (CoT). This is when we provide some additional text in the prompt that helps the model elaborate more on the answer before arriving at the final result. In this task with Yelp, it doesn't seem to help much, but we’ll demonstrate how to do it with LangChain anyway because it’s a useful tool.\n","\n","We’ll use the simplest version of CoT by adding the phrase \"Let's think step by step\" to our prompt. This will encourage the model to elaborate more, but it will also ruin the JSON format. So, we’ll need to pass the output again to the LLM and ask it to build the answer in JSON format; this is done by creating two chains and passing one to the other:"],"metadata":{"id":"B__X-ytrNdmn"}},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# first prompt with elaboration and prediction\n","prompt_1 = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"Here is a review of some company:\n","\n","            \"{review}\"\n","\n","            You need to analyse the sentiment of this text. Then make a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","            Let's think step by step.\n","        \"\"\")\n",")\n","\n","# chain with simple string output\n","chain_1 = prompt_1 | llm | StrOutputParser()\n","\n","# second prompt just for JSON extraction\n","prompt_2 = ChatPromptTemplate.from_template(\n","    textwrap.dedent(\"\"\"\n","            Here is the analysis of the sentiment of a user review:\n","\n","            \"{cot}\"\n","\n","            Return a JSON object with a `rating` key that stores a\n","            prediction of how many stars from 1 to 5 this user rated this company.\n","        \"\"\")\n",")\n","\n","# second chain using syntax for passing results from one chain to another\n","chain_2 = {\"cot\": chain_1} | prompt_2 | llm | json_parser"],"metadata":{"id":"03EnKpOTUCjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test how it works:"],"metadata":{"id":"4jorv8Pe9mZh"}},{"cell_type":"code","source":["chain_2.invoke({\"review\": dataset['train'][0]['text']})"],"metadata":{"id":"8ssubJ_hR5GL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aacc, fail = evaluate_chatgpt_predictions_2(dataset['test'], chain_2, subset_size=100)\n","print()\n","print(\"Accuracy: \", acc)\n","print(\"Failed: \", fail)"],"metadata":{"id":"rLPZw4xJSgfX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bonus exercise.** Can you make use of this technique to actually improve the quality? Here’s [a guide on CoT](https://deepgram.com/learn/chain-of-thought-prompting-guide) that you can peruse if needed.\n","\n","_Note: as the change in quality decreases, you’ll need to use more examples for testing._"],"metadata":{"id":"ouQS6nbP7ibu"}},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1wkxdhtNHUm3S3aAbq4nK586mor8YIv5l\" width=320 /></center>"],"metadata":{"id":"p1CtPwPMXRdo"}},{"cell_type":"markdown","source":["## RAG with LangChain"],"metadata":{"id":"ITDcwTDJZAih"}},{"cell_type":"markdown","source":["Just to see how powerful LangChain is, let's implement another thing using it: **Retrieval Augmented Generation**, or **RAG**. The idea behind this approach is to improve the LLMs question-answering abilities by adding search results from a search engine or a database as additional information. This way, we utilize both the LLMs ability to generate intelligent answers, and we also make sure that the information it uses is as up-to-date as possible.\n","\n","A simple RAG in LangChain will work almost out-of-the-box, so let's make one, and get a general overview of how it works.\n","\n","We’ll need to know about two new concepts for this task: **agents** and **tools**. Essentially, an agent is a high-level system that uses an LLM together with prompt templates and various tools to solve complex tasks. Tools are things like web search, Python code, various APIs (also known as \"plugins\" in ChatGPT web UI).\n","\n","So, in our case, we’ll just need to create an agent, and provide it with a web search tool and the appropriate prompts. We’ll make use DuckDuckGo search and a pre-made prompt from LangChain Hub designed for these kinds of agents:"],"metadata":{"id":"1vLpP9bjeHzp"}},{"cell_type":"code","source":["!pip install -qq duckduckgo-search langchainhub langchain-community"],"metadata":{"id":"IyPuxnrj8Pr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.agents import AgentExecutor, create_react_agent\n","from langchain import hub\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","# just loading a prompt for ReAct agents (Reasoning + Acting)\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# using only one tool: web search\n","tools = [DuckDuckGoSearchRun()]\n","\n","# pass everything to an agent (llm was already defined before)\n","agent = create_react_agent(llm, tools, prompt)\n","# just another wrapper, but we are setting verbose to true to see the details\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"],"metadata":{"id":"_XVieslPfyhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's run the agent with a query that involves looking up some up-to-date information:"],"metadata":{"id":"L4UCSYd7BWrs"}},{"cell_type":"code","source":["agent_executor.invoke({\n","    \"input\": \"What is the most interesting scientific discovery of 2024?\"})"],"metadata":{"id":"ik0GG-FLge8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the output you can see that the agent:\n","\n","\n","\n","1. Understood what it needs to search for, and created a search query\n","2. Got the search results from the search engine\n","3. Analyzed the results and generated the answer\n","\n","So, there’s a simple way to upgrade a search engine with LLMs, but even more complex RAG's are getting lots of action in many applications right now!"],"metadata":{"id":"MAMwqKM4Drq0"}},{"cell_type":"markdown","source":["# 6. Conclusion"],"metadata":{"id":"367f8e3QaDik"}},{"cell_type":"markdown","source":["In this lesson, we talked a lot about ChatGPT and its applications, so let's wrap up what we've learned:\n","\n","\n","\n","* We understood how the GPT model is built using **transformer** architecture and how ChatGPT is built on top of it\n","* We learned about different kinds of model learning, and most interestingly for LLMs, **in-context learning**\n","* We practised writing applications on top of LLMs, using both a basic **API** and the **LangChain** library\n","\n","Up until this point, we’ve mostly been talking about NLP tasks, but in the **next lesson,** we’ll switch to a new kind of task: **image classification**."],"metadata":{"id":"sKz0ca90aG4I"}},{"cell_type":"markdown","source":["By the way, if you’re interested, here are some additional resources on the things we talked about in this lesson:\n","\n","* This [blog post from OpenAI](https://openai.com/blog/chatgpt) describes the ChatGPT model\n","* See a [GPT-2 implementation](https://jaykmody.com/blog/gpt-from-scratch/) from scratch in NumPy\n","* A [guide](https://deepgram.com/learn/chain-of-thought-prompting-guide) on the Chain-of-Thought technique\n","* A [simple guide](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain) on building RAGs for more specific applications than web search"],"metadata":{"id":"wP7uohwtWK4a"}}]}